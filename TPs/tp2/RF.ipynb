{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H78qTC9BlZMq",
        "AsBnofU0ldDm",
        "tCuoqhFYhBZY",
        "ScpDtm8_omtl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/TPs/tp2/RF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de librerias y modelos"
      ],
      "metadata": {
        "id": "H78qTC9BlZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ebooklib beautifulsoup4 pandas\n",
        "#!pip install -U \"huggingface_hub<0.20.0\" \"transformers<4.38\"\n",
        "!pip install -U transformers huggingface_hub datasets accelerate sentence-transformers\n"
      ],
      "metadata": {
        "id": "9Wct7dMGqvR7",
        "outputId": "ea227d63-5359-49b3-e5ba-c54f49a829a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.37.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.19.4)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-1.1.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, huggingface_hub, tokenizers, transformers, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.2\n",
            "    Uninstalling transformers-4.37.2:\n",
            "      Successfully uninstalled transformers-4.37.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed datasets-4.4.1 huggingface_hub-0.36.0 pyarrow-22.0.0 tokenizers-0.22.1 transformers-4.57.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LJ3TrxZflROM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from ebooklib import epub\n",
        "#import ebooklib\n",
        "#from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RKHUgiJplYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de funciones"
      ],
      "metadata": {
        "id": "AsBnofU0ldDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' ]+\", ' ', t)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*e\\s*\\.?\\s*d\\s*\\.?\\s*d\\s*\\.?\\s*o\\s*\\.?', 'peddo', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r's\\s*\\.?\\s*p\\s*\\.?\\s*a\\s*\\.?\\s*d\\s*\\.?\\s*a\\.?', 'spada', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*s\\s*\\.?', 'ps', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'a\\s*\\.?\\s*c\\s*\\.?', 'ac', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "abGSiELDq-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r'\\.\\.\\.', ' ', t)\n",
        "    t = re.sub(r'\\.\\.', ' ', t)\n",
        "    t = re.sub(r'\\,\\,', ' ', t)\n",
        "    t = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚ0-9¿?,.' ]+\", ' ', t)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*e\\s*\\.?\\s*d\\s*\\.?\\s*d\\s*\\.?\\s*o\\s*\\.?', 'PEDDO', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r's\\s*\\.?\\s*p\\s*\\.?\\s*a\\s*\\.?\\s*d\\s*\\.?\\s*a\\.?', 'SPADA', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*s\\s*\\.?', 'PS', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'a\\s*\\.?\\s*c\\s*\\.?', 'AC', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "L41yJLZ-rFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_pd(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")\n",
        "\n",
        "  f = pd.read_csv('libro_parrafos.csv')\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  return parrafos"
      ],
      "metadata": {
        "id": "y48N-5J8s7We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoria_gramatical_stanza(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    token = doc.sentences[0].words[0]\n",
        "    return token.upos\n",
        "\n",
        "upos2id = {\n",
        "    \"NOUN\": 0, #Sustantivo común. Ej: gato, casa, libro, profesor\n",
        "    \"PROPN\": 1, #Sustantivo propio. Ej: Argentina, Azul, Google\n",
        "    \"VERB\": 2, #Verbo léxico. Ej: comer, hablar, correr\n",
        "    \"ADJ\": 3, #Adjetivo. Ej: rápido, azul, brillante\n",
        "    \"ADV\": 4, #Adverbio. Ej: rápidamente, muy, cerca\n",
        "    \"PRON\": 5, #Pronombre. Ej: yo, tú, él, eso, alguien\n",
        "    \"DET\": 6, #Determinante / artículo. Ej: el, la, los, un, ese, mi\n",
        "    \"ADP\": 7, #Adposición: preposición o posposición. Ej: de, para, con, sin, sobre\n",
        "    \"SCONJ\": 8, #Conjunción subordinante. Ej: que, porque, aunque, si\n",
        "    \"CCONJ\": 9, #Conjunción coordinante. Ej: y, o, pero, ni\n",
        "    \"NUM\": 10, #Numeral. Ej: uno, dos, 50, tercero\n",
        "    \"INTJ\": 11, #Interjección. Ej: ay!, hola!, uf, eh\n",
        "    \"PART\": 12, #Partícula gramatical (raro en español). Ejemplos típicos en inglés (not, 's), en español casi no se usa, pero aparece en casos como \"sí\" enfático.\n",
        "    \"AUX\": 13, #Verbo auxiliar. Ej: haber, ser (cuando forman tiempos compuestos: “he comido”, “está hablando”)\n",
        "    \"PUNCT\": 14, #Signos de puntuación. Ej: , . ; ! ?\n",
        "    \"SYM\": 15, #Símbolos. Ej: $, %, +, =, →\n",
        "    \"X\": 16 #Otros / desconocidos / extranjeros. Cualquier cosa que no encaja en ninguna categoría.\n",
        "}\n",
        "\n",
        "def indice_categoria_stanza(palabra):\n",
        "    pos = categoria_gramatical_stanza(palabra)\n",
        "    return upos2id.get(pos,-1)"
      ],
      "metadata": {
        "id": "yXHgA4Nke0dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFSinEtiquetas(parrafo):\n",
        "  data_set_RF_sin_etiquetas = pd.DataFrame(columns = ['instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte'])\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "\n",
        "  for k, parrafo in enumerate(parrafo):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      #categoria = indice_categoria_stanza(palabra)\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria)\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        instancia_ids.append(instancia_id)\n",
        "\n",
        "  data_set_RF_sin_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_sin_etiquetas['token'] = tokens\n",
        "  data_set_RF_sin_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_sin_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_sin_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_sin_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_sin_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_sin_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_sin_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_sin_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_sin_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_sin_etiquetas['forma_parte'] = forman_parte\n",
        "\n",
        "  return data_set_RF_sin_etiquetas"
      ],
      "metadata": {
        "id": "saVcamHuERO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFConEtiquetas(parrafos):\n",
        "  data_set_RF_con_etiquetas = pd.DataFrame(columns = ['palabra_default', 'instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte', 'punt_inicial', 'punt_final', 'capitalización'])\n",
        "\n",
        "  palabras_default = []\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "  puntuaciones_iniciales = []\n",
        "  puntuaciones_finales = []\n",
        "  capitalizaciones = []\n",
        "\n",
        "  datos_limpios = parrafos['limpio']\n",
        "  datos_default = parrafos['default']\n",
        "  for k, parrafo in enumerate(datos_limpios):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "\n",
        "    palabras_d = datos_default.iloc[k].split()\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split('.') if (palabra.count('.') > 1 or ('.' in palabra and palabra[-1] != '.' and palabra[0] != '.')) else [palabra])]\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split(',') if (palabra.count(',') > 1 or (',' in palabra and palabra[-1] != ',' and palabra[0] != ',')) else [palabra])]\n",
        "\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split('?') if (palabra.count('?') > 1 or ('?' in palabra and palabra[-1] != '?' and palabra[0] != '?')) else [palabra])]\n",
        "\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split('¿') if (palabra.count('¿') > 1 or ('¿' in palabra and palabra[-1] != '¿' and palabra[0] != '¿')) else [palabra])]\n",
        "    palabras_d = [palabra for palabra in palabras_d if (palabra != '')]\n",
        "\n",
        "\n",
        "    npal = 0\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      palabra_default = ' '\n",
        "\n",
        "\n",
        "      if palabra == \"'\": #or palabra == 'richdad' or palabra == 'www' or palabra == 'com' or palabra == 'infolibros' or palabra == 'org':\n",
        "        continue\n",
        "\n",
        "      #if palabras_d[npal] == 'www.richdad.com' or palabras_d[npal] == 'richdad.com' or palabras_d[npal] == 'InfoLibros.org':\n",
        "      #  npal += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if npal == len(palabras_d)-1:\n",
        "        #print(palabras_d, npal)\n",
        "        print(instancia_id)\n",
        "      while palabras_d[npal] == '?' or palabras_d[npal] == '¿' or palabras_d[npal] == '.' or palabras_d[npal] == ',' :\n",
        "        if palabras_d[npal] == '¿':\n",
        "          inicio_pregunta = True\n",
        "        npal += 1\n",
        "\n",
        "      if palabras_d[npal] != '?' and palabras_d[npal] != '¿' and palabras_d[npal] != '.' and palabras_d[npal] != ',':\n",
        "        palabra_default = palabras_d[npal]\n",
        "\n",
        "      if palabra_default[0] == \"¿\":\n",
        "        inicio_pregunta = True\n",
        "\n",
        "      if palabra_default[-1] == \"?\":\n",
        "        puntuacion_final = 3\n",
        "      elif palabra_default[-1] == \".\":\n",
        "        puntuacion_final = 1\n",
        "      elif palabra_default[-1] == \",\":\n",
        "        puntuacion_final = 2\n",
        "      else:\n",
        "        if npal != len(palabras_d)-1:\n",
        "          if palabras_d[npal+1] == '?':\n",
        "            puntuacion_final = 3\n",
        "          elif palabras_d[npal+1] == '.':\n",
        "            puntuacion_final = 1\n",
        "          elif palabras_d[npal+1] == ',':\n",
        "            puntuacion_final = 2\n",
        "          else:\n",
        "            puntuacion_final = 0\n",
        "        else:\n",
        "          puntuacion_final = 0\n",
        "\n",
        "      #categoria = indice_categoria_stanza(palabra)\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      if palabra_default.islower():\n",
        "        capitalizacion = 0\n",
        "      elif palabra_default.istitle():\n",
        "        capitalizacion = 1\n",
        "      elif palabra_default.isupper():\n",
        "        capitalizacion = 3\n",
        "      else:\n",
        "        capitalizacion = 2\n",
        "\n",
        "      npal += 1\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria) #cambiar\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuaciones_iniciales.append(1)\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuaciones_iniciales.append(0)\n",
        "\n",
        "        if j == n_tok - 1:\n",
        "          puntuaciones_finales.append(puntuacion_final)\n",
        "        else:\n",
        "          puntuaciones_finales.append(0)\n",
        "\n",
        "        capitalizaciones.append(capitalizacion)\n",
        "        instancia_ids.append(instancia_id)\n",
        "        palabras_default.append(palabra_default)\n",
        "\n",
        "  data_set_RF_con_etiquetas['palabra_default'] = palabras_default\n",
        "  data_set_RF_con_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_con_etiquetas['token'] = tokens\n",
        "  data_set_RF_con_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_con_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_con_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_con_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_con_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_con_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_con_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_con_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_con_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_con_etiquetas['forma_parte'] = forman_parte\n",
        "  data_set_RF_con_etiquetas['punt_inicial'] = puntuaciones_iniciales\n",
        "  data_set_RF_con_etiquetas['punt_final'] = puntuaciones_finales\n",
        "  data_set_RF_con_etiquetas['capitalización'] = capitalizaciones\n",
        "\n",
        "  return data_set_RF_con_etiquetas"
      ],
      "metadata": {
        "id": "kdihneKxf8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos y creación de data set"
      ],
      "metadata": {
        "id": "Bi10oO7bd2wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'\n",
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'\n",
        "path = path + libro1"
      ],
      "metadata": {
        "id": "v6C-6HTBsg24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2WkOAYegsoBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos = convertir_epub_a_pd('libro1.epub')"
      ],
      "metadata": {
        "id": "n4DpCsmosqjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data set sin categoria gramatical"
      ],
      "metadata": {
        "id": "hH5Gi31ZJme2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas = crearDataSetRFConEtiquetas(parrafos)\n",
        "#dataSet_RF_sin_etiquetas = crearDataSetRFSinEtiquetas(parrafos['limpio'])"
      ],
      "metadata": {
        "id": "QTLUC6E1HqtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas = crearDataSetRFConEtiquetas(parrafos)\n",
        "#dataSet_RF_sin_etiquetas = crearDataSetRFSinEtiquetas(parrafos['limpio'])"
      ],
      "metadata": {
        "id": "ywTmyS5UtJg8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data set con categoria gramatical"
      ],
      "metadata": {
        "id": "mNUYPOc6Jt89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'\n",
        "datos = '/dataSet_RF_con_etiquetas.csv'\n",
        "path = path + datos"
      ],
      "metadata": {
        "id": "zYK5E6LLIMJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O dataSet_RF_con_etiquetas.csv $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GGOLWUJAIntT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas_cg = pd.read_csv('dataSet_RF_con_etiquetas.csv')"
      ],
      "metadata": {
        "id": "b6yTKsbMrgwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos_final = pd.read_csv('dataset_parrafos_total3.csv')\n",
        "parrafos_final = parrafos_final.dropna()\n",
        "parrafos_final['limpio'] = parrafos_final['limpio'].apply(normalize_text_X)\n",
        "parrafos_final['default'] = parrafos_final['default'].apply(normalize_text_y)\n",
        "parrafos_final.reset_index(drop=True)\n",
        "parrafos_final.drop(index=[7764, 9497, 9498], inplace=True) #palabra.¿palabra parrafo raro, parrafo raro\n",
        "parrafos_final.reset_index(drop=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MIZu0Q-Gqy_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas_final = crearDataSetRFConEtiquetas(parrafos_final)\n",
        "dataSet_RF_con_etiquetas_final = pd.read_csv('/content/drive/MyDrive/dataSet_RF_con_etiquetas_final.csv')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HCLImXFryIIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 1: capitalización\n",
        "* 0: todo en minúsculas (ej: “hola”)\n",
        "* 1: primera letra en mayúscula (ej: “Hola”) (incluye palabras de 1 letra)\n",
        "* 2: algunas (pero no todas) letras en mayúscula (ej: “McDonald's”  “iPhone”)\n",
        "* 3: todo en mayúsculas (ej.: “ONU”, “NASA”, “UBA”) (más de una letra)"
      ],
      "metadata": {
        "id": "Ehs4pzDL1miS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 2: puntuación inicial\n",
        "* 0: sin puntuación.\n",
        "* 1: \"?\""
      ],
      "metadata": {
        "id": "FSWw3oCTgdig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 3: puntuación final\n",
        "* 0: sin puntuación.\n",
        "* 1: \".\"\n",
        "* 2: \",\"\n",
        "* 3: \"?\""
      ],
      "metadata": {
        "id": "VGVr6l9gguUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separación en train y test"
      ],
      "metadata": {
        "id": "tCuoqhFYhBZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataSet_RF_con_etiquetas[dataSet_RF_con_etiquetas['instancia_id'] <= 2972]\n",
        "test = dataSet_RF_con_etiquetas[dataSet_RF_con_etiquetas['instancia_id'] > 2972]"
      ],
      "metadata": {
        "id": "CYjUNCywwPpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cg = dataSet_RF_con_etiquetas_cg[dataSet_RF_con_etiquetas_cg['instancia_id'] <= 2972]\n",
        "test_cg = dataSet_RF_con_etiquetas_cg[dataSet_RF_con_etiquetas_cg['instancia_id'] > 2972]"
      ],
      "metadata": {
        "id": "v4Frh7O-IzuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train.drop(columns=['palabra_default', 'token', 'categoria_gramatical','capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_train = train[['capitalización', 'punt_inicial', 'punt_final']]\n",
        "X_test = test.drop(columns=['palabra_default', 'token', 'categoria_gramatical', 'capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_test = test[['capitalización', 'punt_inicial', 'punt_final']]"
      ],
      "metadata": {
        "id": "YdSBC6C4ynNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_cg = train_cg.drop(columns=['palabra_default', 'token', 'capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_train_cg = train_cg[['capitalización', 'punt_inicial', 'punt_final']]\n",
        "X_test_cg = test_cg.drop(columns=['palabra_default', 'token',  'capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_test_cg = test_cg[['capitalización', 'punt_inicial', 'punt_final']]"
      ],
      "metadata": {
        "id": "HIE4IBzbI98h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sin etiqueta gramatical\n"
      ],
      "metadata": {
        "id": "2D71grVkKoKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 1"
      ],
      "metadata": {
        "id": "ScpDtm8_omtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train1 = y_train['capitalización']\n",
        "y_test1 = y_test['capitalización']"
      ],
      "metadata": {
        "id": "RMa8CWBvzQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train, y_train1)"
      ],
      "metadata": {
        "id": "s-1g2bDCx3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "CWF43puSz8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test1, y_pred1, average = \"macro\")"
      ],
      "metadata": {
        "id": "Bd0pipyx3ftv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm1 = confusion_matrix(y_test1,y_pred1, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "r1kf-Jt71ITx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm1, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title(\"Matriz de confusión para capitalización sin CG\")   # ← Título\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hja5Pe_fLlvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 2"
      ],
      "metadata": {
        "id": "KRFYt6XOhJIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train2 = y_train['punt_inicial']\n",
        "y_test2 = y_test['punt_inicial']"
      ],
      "metadata": {
        "id": "so4LY8NK2eh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train, y_train2)"
      ],
      "metadata": {
        "id": "QQQhKJ7q2kGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "QXW0Fid82oRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test2, y_pred2, average = \"macro\")"
      ],
      "metadata": {
        "id": "6EByF3Px2q0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm2 = confusion_matrix(y_test2, y_pred2, labels=[0,1])"
      ],
      "metadata": {
        "id": "8ewx5ROy2t_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para punt_inicial sin CG')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gv3qAUm1MCVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 3"
      ],
      "metadata": {
        "id": "c-YY-UO2hKT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train3 = y_train['punt_final']\n",
        "y_test3 = y_test['punt_final']"
      ],
      "metadata": {
        "id": "tvXCw98Y3jyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train, y_train3)"
      ],
      "metadata": {
        "id": "HLMzxX8v3n5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "yKRrST3o3rU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test3, y_pred3, average = \"macro\")"
      ],
      "metadata": {
        "id": "jN6H6km53tQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm3 = confusion_matrix(y_test3, y_pred3, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "rGVFU5lL3vlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm3, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para punt_final sin CG')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LJ1GLfZLMpSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Con etiqueta gramatical\n"
      ],
      "metadata": {
        "id": "PI9fNeZjMeeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 1"
      ],
      "metadata": {
        "id": "antWSRQWMhdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train4_cg = y_train_cg['capitalización']\n",
        "y_test4_cg = y_test_cg['capitalización']"
      ],
      "metadata": {
        "id": "prwWVtryMt-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train_cg, y_train4_cg)"
      ],
      "metadata": {
        "id": "Yk0dwpE0M-VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred4_cg = model.predict(X_test_cg)"
      ],
      "metadata": {
        "id": "baToxMYsNC_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test4_cg, y_pred4_cg, average = \"macro\")"
      ],
      "metadata": {
        "id": "2XW7YLU5NImO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm4 = confusion_matrix(y_test4_cg, y_pred4_cg, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "8f7lHLrjNteb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm4, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para capitalización con CG')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Op-rmM-mN2_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 2"
      ],
      "metadata": {
        "id": "qmXFUdYKOEF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train5_cg = y_train_cg['punt_inicial']\n",
        "y_test5_cg = y_test_cg['punt_inicial']"
      ],
      "metadata": {
        "id": "Q5av3Bd4OHa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train_cg, y_train5_cg)"
      ],
      "metadata": {
        "id": "Y0CSTg2DORAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred5_cg = model.predict(X_test_cg)"
      ],
      "metadata": {
        "id": "TsR-0hldOUsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test5_cg, y_pred5_cg, average = \"macro\")"
      ],
      "metadata": {
        "id": "X6JhbvIOOXVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm5 = confusion_matrix(y_test5_cg, y_pred5_cg, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "WtOfHMH3Oy-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm5, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para punt_inicial con CG')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t90Xq_wBO7fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 3"
      ],
      "metadata": {
        "id": "cBcE55aTPlOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train6_cg = y_train_cg['punt_final']\n",
        "y_test6_cg = y_test_cg['punt_final']"
      ],
      "metadata": {
        "id": "459tyS83Pn52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train_cg, y_train6_cg)"
      ],
      "metadata": {
        "id": "IwXav5k-PtpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred6_cg = model.predict(X_test_cg)"
      ],
      "metadata": {
        "id": "gMUHC0TKPxR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test6_cg, y_pred6_cg, average = \"macro\")"
      ],
      "metadata": {
        "id": "HeIUZub5QEcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm6 = confusion_matrix(y_test6_cg, y_pred6_cg, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "moSGQR8LQHOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm6, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para punt_final con CG')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Map0FfyFRMXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}