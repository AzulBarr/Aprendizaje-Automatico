{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H78qTC9BlZMq",
        "AsBnofU0ldDm",
        "Bi10oO7bd2wU",
        "tCuoqhFYhBZY"
      ],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/TPs/tp2/RF_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de librerias y modelos"
      ],
      "metadata": {
        "id": "H78qTC9BlZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ebooklib beautifulsoup4 pandas\n",
        "!pip install -U transformers huggingface_hub datasets accelerate sentence-transformers"
      ],
      "metadata": {
        "id": "9Wct7dMGqvR7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ3TrxZflROM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RKHUgiJplYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de funciones"
      ],
      "metadata": {
        "id": "AsBnofU0ldDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruccion_texto(dataSet): #data set con columnas: instancia_id (cte), token_id, token, capitalización, punt_inicial, punt_final\n",
        "  frase = ''\n",
        "  frase_final = ''\n",
        "  n = dataSet.shape[0]\n",
        "  i = 0\n",
        "  capitalizaciones = []\n",
        "  punt_finales = []\n",
        "  punt_iniciales = []\n",
        "\n",
        "  for fila in dataSet.iterrows():\n",
        "    subpalabra = fila[1]['token']\n",
        "\n",
        "    if subpalabra[:2] == '##':\n",
        "      subpalabra = subpalabra[2:]\n",
        "    else:\n",
        "      capitalizaciones.append(fila[1]['capitalizacion'])\n",
        "      punt_iniciales.append(fila[1]['punt_inicial'])\n",
        "\n",
        "    if i == n-1:\n",
        "      frase += subpalabra\n",
        "      punt_finales.append(fila[1]['punt_final'])\n",
        "    else:\n",
        "      if dataSet.iloc[i+1]['token'][:2] == '##':\n",
        "        frase += subpalabra\n",
        "      else:\n",
        "        frase += subpalabra + ' '\n",
        "        punt_finales.append(fila[1]['punt_final'])\n",
        "    i +=1\n",
        "\n",
        "  palabras = frase.split()\n",
        "  for i, palabra in enumerate(palabras):\n",
        "\n",
        "    if punt_iniciales[i] == 1:\n",
        "      frase_final += '¿'\n",
        "\n",
        "    if capitalizaciones[i] == 0:\n",
        "      frase_final += palabra\n",
        "    elif capitalizaciones[i] == 1:\n",
        "      frase_final += palabra.capitalize()\n",
        "    elif capitalizaciones[i] == 3:\n",
        "      frase_final += palabra.upper()\n",
        "    else:\n",
        "      frase_final += palabra[:-1].capitalize() + palabra[-1].upper()\n",
        "\n",
        "    if punt_finales[i] == 1:\n",
        "      frase_final += '.'\n",
        "    elif punt_finales[i] == 2:\n",
        "      frase_final += ','\n",
        "    elif punt_finales[i] == 3:\n",
        "      frase_final += '?'\n",
        "\n",
        "\n",
        "\n",
        "    if i != len(palabras) - 1 and palabras[i+1] != \"'\" and palabras[i] != \"'\":\n",
        "        frase_final += ' '\n",
        "\n",
        "  print(frase_final)"
      ],
      "metadata": {
        "id": "EziMHbd5Abwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = re.sub(r'(?<!\\w)A\\.C\\.(?!\\w)', 'a c', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'(?<!\\w)P\\.S\\.(?!\\w)', 'p s', t, flags=re.IGNORECASE)\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' ]+\", ' ', t)\n",
        "    #t = re.sub(r'p\\s*\\.?\\s*s\\s*\\.?', 'ps', t, flags=re.IGNORECASE)\n",
        "    #t = re.sub(r'a\\s*\\.?\\s*c\\s*\\.?', 'ac', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "abGSiELDq-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r'\\.\\.\\.', ' ', t)\n",
        "    t = re.sub(r'\\.\\.', ' ', t)\n",
        "    t = re.sub(r'\\,\\,', ' ', t)\n",
        "    t = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚ0-9¿?,.' ]+\", ' ', t)\n",
        "    #t = re.sub(r'p\\s*\\.?\\s*s\\s*\\.?', 'PS', t, flags=re.IGNORECASE)\n",
        "    #t = re.sub(r'a\\s*\\.?\\s*c\\s*\\.?', 'AC', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'(?<!\\w)P\\.S\\.(?!\\w)', 'P S', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'(?<!\\w)A\\.C\\.(?!\\w)', 'A C', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "L41yJLZ-rFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFSinEtiquetas(parrafos):\n",
        "  data_set_RF_sin_etiquetas = pd.DataFrame(columns = ['instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte'])\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "\n",
        "  datos_limpios = parrafos\n",
        "  for k, parrafo in enumerate(datos_limpios):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      if palabra == \"'\":\n",
        "        continue\n",
        "\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        instancia_ids.append(instancia_id)\n",
        "\n",
        "  data_set_RF_sin_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_sin_etiquetas['token'] = tokens\n",
        "  data_set_RF_sin_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_sin_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_sin_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_sin_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_sin_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_sin_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_sin_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_sin_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_sin_etiquetas['forma_parte'] = forman_parte\n",
        "\n",
        "  return data_set_RF_sin_etiquetas"
      ],
      "metadata": {
        "id": "saVcamHuERO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFConEtiquetas(parrafos):\n",
        "  data_set_RF_con_etiquetas = pd.DataFrame(columns = ['palabra_default', 'instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte', 'punt_inicial', 'punt_final', 'capitalización'])\n",
        "\n",
        "  palabras_default = []\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "  puntuaciones_iniciales = []\n",
        "  puntuaciones_finales = []\n",
        "  capitalizaciones = []\n",
        "\n",
        "  datos_limpios = parrafos['limpio']\n",
        "  datos_default = parrafos['default']\n",
        "  for k, parrafo in enumerate(datos_limpios):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "\n",
        "    palabras_d = datos_default.iloc[k].split()\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split('.') if (palabra.count('.') > 1 or ('.' in palabra and palabra[-1] != '.' and palabra[0] != '.')) else [palabra])]\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split(',') if (palabra.count(',') > 1 or (',' in palabra and palabra[-1] != ',' and palabra[0] != ',')) else [palabra])]\n",
        "\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split('?') if (palabra.count('?') > 1 or ('?' in palabra and palabra[-1] != '?' and palabra[0] != '?')) else [palabra])]\n",
        "\n",
        "    palabras_d = [subpalabra\n",
        "                for palabra in palabras_d\n",
        "                for subpalabra in (palabra.split('¿') if (palabra.count('¿') > 1 or ('¿' in palabra and palabra[-1] != '¿' and palabra[0] != '¿')) else [palabra])]\n",
        "    palabras_d = [palabra for palabra in palabras_d if (palabra != '')]\n",
        "\n",
        "\n",
        "    npal = 0\n",
        "    for i, palabra in enumerate(palabras):\n",
        "\n",
        "      palabra_default = ' '\n",
        "\n",
        "      if palabra == \"'\":\n",
        "        continue\n",
        "\n",
        "      while palabras_d[npal] == '?' or palabras_d[npal] == '¿' or palabras_d[npal] == '.' or palabras_d[npal] == ',' :\n",
        "        if palabras_d[npal] == '¿':\n",
        "          inicio_pregunta = True\n",
        "        npal += 1\n",
        "\n",
        "      if palabras_d[npal] != '?' and palabras_d[npal] != '¿' and palabras_d[npal] != '.' and palabras_d[npal] != ',':\n",
        "        palabra_default = palabras_d[npal]\n",
        "\n",
        "      if palabra_default[0] == \"¿\":\n",
        "        inicio_pregunta = True\n",
        "\n",
        "      if palabra_default[-1] == \"?\":\n",
        "        puntuacion_final = 3\n",
        "      elif palabra_default[-1] == \".\":\n",
        "        puntuacion_final = 1\n",
        "      elif palabra_default[-1] == \",\":\n",
        "        puntuacion_final = 2\n",
        "      else:\n",
        "        if npal != len(palabras_d)-1:\n",
        "          if palabras_d[npal+1] == '?':\n",
        "            puntuacion_final = 3\n",
        "          elif palabras_d[npal+1] == '.':\n",
        "            puntuacion_final = 1\n",
        "          elif palabras_d[npal+1] == ',':\n",
        "            puntuacion_final = 2\n",
        "          else:\n",
        "            puntuacion_final = 0\n",
        "        else:\n",
        "          puntuacion_final = 0\n",
        "\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      if palabra_default.islower():\n",
        "        capitalizacion = 0\n",
        "      elif palabra_default.istitle():\n",
        "        capitalizacion = 1\n",
        "      elif palabra_default.isupper():\n",
        "        capitalizacion = 3\n",
        "      else:\n",
        "        capitalizacion = 2\n",
        "\n",
        "      npal += 1\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria)\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuaciones_iniciales.append(1)\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuaciones_iniciales.append(0)\n",
        "\n",
        "        if j == n_tok - 1:\n",
        "          puntuaciones_finales.append(puntuacion_final)\n",
        "        else:\n",
        "          puntuaciones_finales.append(0)\n",
        "\n",
        "        capitalizaciones.append(capitalizacion)\n",
        "        instancia_ids.append(instancia_id)\n",
        "        palabras_default.append(palabra_default)\n",
        "      print(instancia_id)\n",
        "\n",
        "\n",
        "  data_set_RF_con_etiquetas['palabra_default'] = palabras_default\n",
        "  data_set_RF_con_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_con_etiquetas['token'] = tokens\n",
        "  data_set_RF_con_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_con_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_con_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_con_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_con_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_con_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_con_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_con_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_con_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_con_etiquetas['forma_parte'] = forman_parte\n",
        "  data_set_RF_con_etiquetas['punt_inicial'] = puntuaciones_iniciales\n",
        "  data_set_RF_con_etiquetas['punt_final'] = puntuaciones_finales\n",
        "  data_set_RF_con_etiquetas['capitalización'] = capitalizaciones\n",
        "\n",
        "  return data_set_RF_con_etiquetas"
      ],
      "metadata": {
        "id": "kdihneKxf8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de datos y creación de data set"
      ],
      "metadata": {
        "id": "Bi10oO7bd2wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parrafos_final = pd.read_csv('dataset_parrafos_total3.csv')"
      ],
      "metadata": {
        "id": "43-Hq1bzrFph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Ya está el dataSet armado\n",
        "\n",
        "parrafos_final = pd.read_csv('dataset_parrafos_total3.csv')\n",
        "parrafos_final = parrafos_final.dropna()\n",
        "parrafos_final['limpio'] = parrafos_final['limpio'].apply(normalize_text_X)\n",
        "parrafos_final['default'] = parrafos_final['default'].apply(normalize_text_y)\n",
        "parrafos_final.reset_index(drop=True)\n",
        "parrafos_final.drop(index=[7764, 9497, 9498], inplace=True) #palabra.¿palabra parrafo raro, parrafo raro\n",
        "parrafos_final.reset_index(drop=True)\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MIZu0Q-Gqy_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas_final = crearDataSetRFConEtiquetas(parrafos_final)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m6-VOO86nI3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "VhOlLIN7uBaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas_final.to_csv('/content/drive/MyDrive/dataSet_RF_con_etiquetas_final.csv', index=False)\n"
      ],
      "metadata": {
        "id": "uwLTmOoWv_pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataSet_RF_con_etiquetas_final = crearDataSetRFConEtiquetas(parrafos_final)\n",
        "dataSet_RF_con_etiquetas_final = pd.read_csv('/content/drive/MyDrive/dataSet_RF_con_etiquetas_final.csv')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HCLImXFryIIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 1: capitalización\n",
        "* 0: todo en minúsculas (ej: “hola”)\n",
        "* 1: primera letra en mayúscula (ej: “Hola”) (incluye palabras de 1 letra)\n",
        "* 2: algunas (pero no todas) letras en mayúscula (ej: “McDonald's”  “iPhone”)\n",
        "* 3: todo en mayúsculas (ej.: “ONU”, “NASA”, “UBA”) (más de una letra)"
      ],
      "metadata": {
        "id": "Ehs4pzDL1miS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 2: puntuación inicial\n",
        "* 0: sin puntuación.\n",
        "* 1: \"¿\""
      ],
      "metadata": {
        "id": "FSWw3oCTgdig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 3: puntuación final\n",
        "* 0: sin puntuación.\n",
        "* 1: \".\"\n",
        "* 2: \",\"\n",
        "* 3: \"?\""
      ],
      "metadata": {
        "id": "VGVr6l9gguUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separación en train y test"
      ],
      "metadata": {
        "id": "tCuoqhFYhBZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF_con_etiquetas_final[dataSet_RF_con_etiquetas_final['instancia_id'] <= 24360].shape"
      ],
      "metadata": {
        "id": "VRbrommTGPMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = dataSet_RF_con_etiquetas_final[dataSet_RF_con_etiquetas_final['instancia_id'] <= 20000] #~10%\n",
        "train = dataSet_RF_con_etiquetas_final[dataSet_RF_con_etiquetas_final['instancia_id'] > 20000]"
      ],
      "metadata": {
        "id": "1mF0DDtKHL3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train.drop(columns=['palabra_default', 'token', 'categoria_gramatical','capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_train = train[['capitalización', 'punt_inicial', 'punt_final']]\n",
        "X_test = test.drop(columns=['palabra_default', 'token', 'categoria_gramatical', 'capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_test = test[['capitalización', 'punt_inicial', 'punt_final']]"
      ],
      "metadata": {
        "id": "YdSBC6C4ynNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 1"
      ],
      "metadata": {
        "id": "ScpDtm8_omtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train1 = y_train['capitalización']\n",
        "y_test1 = y_test['capitalización']"
      ],
      "metadata": {
        "id": "RMa8CWBvzQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, min_samples_split=10)\n",
        "#model.fit(X_train, y_train1)"
      ],
      "metadata": {
        "id": "s-1g2bDCx3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joblib.dump(model, \"/content/drive/MyDrive/modelo_RF_capitalizacion.joblib\")"
      ],
      "metadata": {
        "id": "lYCxvuh08i8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load(\"/content/drive/MyDrive/modelo_RF_capitalizacion.joblib\")"
      ],
      "metadata": {
        "id": "A5MfnSIzArbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "CWF43puSz8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test1, y_pred1, average = \"macro\")"
      ],
      "metadata": {
        "id": "Bd0pipyx3ftv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm1 = confusion_matrix(y_test1,y_pred1, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "r1kf-Jt71ITx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm1, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title(\"Matriz de confusión para capitalización\")   # ← Título\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hja5Pe_fLlvl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 2"
      ],
      "metadata": {
        "id": "KRFYt6XOhJIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train2 = y_train['punt_inicial']\n",
        "y_test2 = y_test['punt_inicial']"
      ],
      "metadata": {
        "id": "so4LY8NK2eh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model2 = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, min_samples_split=10)\n",
        "#model2.fit(X_train, y_train2)"
      ],
      "metadata": {
        "id": "QQQhKJ7q2kGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joblib.dump(model2, \"/content/drive/MyDrive/modelo_RF_punt_inicial.joblib\")"
      ],
      "metadata": {
        "id": "fxNv-PNdCezW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = joblib.load(\"/content/drive/MyDrive/modelo_RF_punt_inicial.joblib\")"
      ],
      "metadata": {
        "id": "qEUssWKtCsAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = model2.predict(X_test)"
      ],
      "metadata": {
        "id": "QXW0Fid82oRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test2, y_pred2, average = \"macro\")"
      ],
      "metadata": {
        "id": "6EByF3Px2q0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm2 = confusion_matrix(y_test2, y_pred2, labels=[0,1])"
      ],
      "metadata": {
        "id": "8ewx5ROy2t_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm2, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para punt_inicial')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gv3qAUm1MCVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest para etiqueta 3"
      ],
      "metadata": {
        "id": "c-YY-UO2hKT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train3 = y_train['punt_final']\n",
        "y_test3 = y_test['punt_final']"
      ],
      "metadata": {
        "id": "tvXCw98Y3jyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model3 = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, min_samples_split=10)\n",
        "#model3.fit(X_train, y_train3)"
      ],
      "metadata": {
        "id": "rjw2v9JKDOvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joblib.dump(model3, \"/content/drive/MyDrive/modelo_RF_punt_final.joblib\")"
      ],
      "metadata": {
        "id": "wX6g_-iNCyar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = joblib.load(\"/content/drive/MyDrive/modelo_RF_punt_final.joblib\")"
      ],
      "metadata": {
        "id": "371YzvDPCv2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3 = model3.predict(X_test)"
      ],
      "metadata": {
        "id": "yKRrST3o3rU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test3, y_pred3, average = \"macro\")"
      ],
      "metadata": {
        "id": "jN6H6km53tQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm3 = confusion_matrix(y_test3, y_pred3, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "rGVFU5lL3vlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm3, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicho\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title('Matriz de confusión para punt_final')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LJ1GLfZLMpSu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}