{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IKP6njfDK4e9"
      ],
      "authorship_tag": "ABX9TyPm4g94eMfMNSjCeH5nCYI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/TPs/tp2/tp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wxeGjIn6uDF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nwupEbruutCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones Auxiliares"
      ],
      "metadata": {
        "id": "IKP6njfDK4e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "gYwlSGEGyMqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-Záéíóúüñ0-9¿?,.' -]+\", ' ', t)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "sWI74TYAyWNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_etiqueta(y):\n",
        "  puntuacion_iniciales = []\n",
        "  puntuacion_finales = []\n",
        "  capitalizaciones = []\n",
        "  instancia_id_count = 0\n",
        "  instancia_ids = []\n",
        "  token_ids = []\n",
        "  tokens_l = []\n",
        "\n",
        "  for parrafo in y:\n",
        "    inicio_pregunta = False\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "\n",
        "    for palabra in palabras:\n",
        "      tokens = tokenizer.tokenize(palabra.lower())\n",
        "\n",
        "      for i in range(len(tokens)):\n",
        "        if tokens[i] == \"¿\":\n",
        "          inicio_pregunta = True\n",
        "          continue\n",
        "        if tokens[i] == \"?\" or tokens[i] == \".\" or tokens[i] == \",\":\n",
        "          continue\n",
        "\n",
        "        instancia_ids.append(instancia_id_count)\n",
        "        token_ids.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "        tokens_l.append(tokens[i])\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuacion_iniciales.append('\"¿\"')\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuacion_iniciales.append(\"\")\n",
        "\n",
        "        if i != len(tokens) - 1:\n",
        "\n",
        "          if tokens[i+1] == \"?\":\n",
        "            puntuacion_finales.append('\"?\"')\n",
        "          elif tokens[i+1] == \".\":\n",
        "            puntuacion_finales.append('\".\"')\n",
        "          elif tokens[i+1] == \",\":\n",
        "            puntuacion_finales.append('\",\"')\n",
        "          else:\n",
        "            puntuacion_finales.append(\"\")\n",
        "\n",
        "        else:\n",
        "          puntuacion_finales.append(\"\")\n",
        "\n",
        "        if palabra.islower():\n",
        "          capitalizaciones.append(0)\n",
        "          ultimo_numero = 0\n",
        "        elif palabra.istitle():\n",
        "          capitalizaciones.append(1)\n",
        "          ultimo_numero = 1\n",
        "        elif palabra.isupper():\n",
        "          capitalizaciones.append(3)\n",
        "          ultimo_numero = 3\n",
        "        else:\n",
        "          capitalizaciones.append(2)\n",
        "          ultimo_numero = 2\n",
        "\n",
        "    instancia_id_count += 1\n",
        "  etiquetas = np.stack([token_ids, tokens_l, puntuacion_iniciales, puntuacion_finales, capitalizaciones])\n",
        "  return etiquetas"
      ],
      "metadata": {
        "id": "dWVD-ZHu2LUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_csv(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8ukgJIVyuEjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSet(libro):\n",
        "\n",
        "  datos = pd.DataFrame(columns=['instancia_id', \"default\", \"limpio\", 'token_id', 'token', 'punt_inicial', 'punt_final', 'capitalización'])\n",
        "  datos['instancia_id'] = df.index\n",
        "  datos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  datos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "  datos['token_id'] = np.zeros(len(datos))\n",
        "  datos['token'] = np.zeros(len(datos))\n",
        "  datos['punt_inicial'] = np.zeros(len(datos))\n",
        "  datos['punt_final'] = np.zeros(len(datos))\n",
        "  datos['capitalización'] = np.zeros(len(datos))\n",
        "\n",
        "  i = 0\n",
        "  for p in datos['default']:\n",
        "    etiquetas = transformar_etiqueta([p])\n",
        "    datos['token_id'][i] = etiquetas[0]\n",
        "    datos['token'][i] = etiquetas[1]\n",
        "    datos['punt_inicial'][i] = etiquetas[2]\n",
        "    datos['punt_final'][i] = etiquetas[3]\n",
        "    datos['capitalización'][i] = etiquetas[4]\n",
        "    i += 1\n",
        "  return datos"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V0_eReCWyXzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar libros"
      ],
      "metadata": {
        "id": "ISGg2S2RK0Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Busca libro en formato epub de mi github"
      ],
      "metadata": {
        "id": "P2WDL7S3PtV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'"
      ],
      "metadata": {
        "id": "NxMriymePxNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'"
      ],
      "metadata": {
        "id": "HbrjtNdCP4EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = path + libro1"
      ],
      "metadata": {
        "id": "6LlG5OLZQO8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AwL64C6xIo6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Codigo"
      ],
      "metadata": {
        "id": "4rJ0jhmDQAgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1E4FbKGQzN3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_embedding_de_parrafo(texto: str):\n",
        "  \"\"\"\n",
        "  Devuelve el embedding (estático) para el token.\n",
        "  \"\"\"\n",
        "  tokens = tokenizer.tokenize(texto)\n",
        "  embeddings = []\n",
        "  for token in tokens:\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "      print(f\"❌ El token '{token}' no pertenece al vocabulario de multilingual BERT.\")\n",
        "      return None\n",
        "    embedding_vector = model.embeddings.word_embeddings.weight[token_id]\n",
        "    embeddings.append(embedding_vector)\n",
        "    print(f\"✅ Token: '{token}' | ID: {token_id}\")\n",
        "    print(f\"Embedding shape: {embedding_vector.shape}\")\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "upL3Ja0KQyh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet = crearDataSet('libro1.epub')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2AWjTOSfQc9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nZLeai5HQh6d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}