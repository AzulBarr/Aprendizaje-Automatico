{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H78qTC9BlZMq",
        "AsBnofU0ldDm",
        "tCuoqhFYhBZY",
        "ScpDtm8_omtl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/TPs/tp2/RF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de librerias y modelos"
      ],
      "metadata": {
        "id": "H78qTC9BlZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas\n",
        "!pip install stanza"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Wct7dMGqvR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ3TrxZflROM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import stanza\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RKHUgiJplYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"es\")\n",
        "nlp = stanza.Pipeline(\"es\", processors=\"tokenize,pos\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5lCZKV0Pei6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de funciones"
      ],
      "metadata": {
        "id": "AsBnofU0ldDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*e\\s*\\.?\\s*d\\s*\\.?\\s*d\\s*\\.?\\s*o\\s*\\.?', 'peddo', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r's\\s*\\.?\\s*p\\s*\\.?\\s*a\\s*\\.?\\s*d\\s*\\.?\\s*a\\.?', 'spada', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "abGSiELDq-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚ0-9¿?,.' -]+\", ' ', t)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*e\\s*\\.?\\s*d\\s*\\.?\\s*d\\s*\\.?\\s*o\\s*\\.?', 'PEDDO', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r's\\s*\\.?\\s*p\\s*\\.?\\s*a\\s*\\.?\\s*d\\s*\\.?\\s*a\\.?', 'SPADA', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "L41yJLZ-rFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_pd(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")\n",
        "\n",
        "  f = pd.read_csv('libro_parrafos.csv')\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  return parrafos"
      ],
      "metadata": {
        "id": "y48N-5J8s7We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoria_gramatical_stanza(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    token = doc.sentences[0].words[0]\n",
        "    return token.upos\n",
        "\n",
        "upos2id = {\n",
        "    \"NOUN\": 0, #Sustantivo común. Ej: gato, casa, libro, profesor\n",
        "    \"PROPN\": 1, #Sustantivo propio. Ej: Argentina, Azul, Google\n",
        "    \"VERB\": 2, #Verbo léxico. Ej: comer, hablar, correr\n",
        "    \"ADJ\": 3, #Adjetivo. Ej: rápido, azul, brillante\n",
        "    \"ADV\": 4, #Adverbio. Ej: rápidamente, muy, cerca\n",
        "    \"PRON\": 5, #Pronombre. Ej: yo, tú, él, eso, alguien\n",
        "    \"DET\": 6, #Determinante / artículo. Ej: el, la, los, un, ese, mi\n",
        "    \"ADP\": 7, #Adposición: preposición o posposición. Ej: de, para, con, sin, sobre\n",
        "    \"SCONJ\": 8, #Conjunción subordinante. Ej: que, porque, aunque, si\n",
        "    \"CCONJ\": 9, #Conjunción coordinante. Ej: y, o, pero, ni\n",
        "    \"NUM\": 10, #Numeral. Ej: uno, dos, 50, tercero\n",
        "    \"INTJ\": 11, #Interjección. Ej: ay!, hola!, uf, eh\n",
        "    \"PART\": 12, #Partícula gramatical (raro en español). Ejemplos típicos en inglés (not, 's), en español casi no se usa, pero aparece en casos como \"sí\" enfático.\n",
        "    \"AUX\": 13, #Verbo auxiliar. Ej: haber, ser (cuando forman tiempos compuestos: “he comido”, “está hablando”)\n",
        "    \"PUNCT\": 14, #Signos de puntuación. Ej: , . ; ! ?\n",
        "    \"SYM\": 15, #Símbolos. Ej: $, %, +, =, →\n",
        "    \"X\": 16 #Otros / desconocidos / extranjeros. Cualquier cosa que no encaja en ninguna categoría.\n",
        "}\n",
        "\n",
        "def indice_categoria_stanza(palabra):\n",
        "    pos = categoria_gramatical_stanza(palabra)\n",
        "    return upos2id.get(pos,-1)"
      ],
      "metadata": {
        "id": "yXHgA4Nke0dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFSinEtiquetas(parrafo):\n",
        "  data_set_RF_sin_etiquetas = pd.DataFrame(columns = ['instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte'])\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "\n",
        "  for k, parrafo in enumerate(parrafo):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      #categoria = indice_categoria_stanza(palabra)\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria) #cambiar\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        instancia_ids.append(instancia_id)\n",
        "\n",
        "  data_set_RF_sin_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_sin_etiquetas['token'] = tokens\n",
        "  data_set_RF_sin_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_sin_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_sin_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_sin_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_sin_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_sin_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_sin_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_sin_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_sin_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_sin_etiquetas['forma_parte'] = forman_parte\n",
        "\n",
        "  return data_set_RF_sin_etiquetas"
      ],
      "metadata": {
        "id": "saVcamHuERO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFConEtiquetas(parrafos):\n",
        "  data_set_RF_con_etiquetas = pd.DataFrame(columns = ['palabra_default', 'instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte', 'punt_inicial', 'punt_final', 'capitalización'])\n",
        "\n",
        "  palabras_default = []\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "  puntuaciones_iniciales = []\n",
        "  puntuaciones_finales = []\n",
        "  capitalizaciones = []\n",
        "\n",
        "  datos_limpios = parrafos['limpio']\n",
        "  datos_default = parrafos['default']\n",
        "  for k, parrafo in enumerate(datos_limpios):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "    palabras_d = datos_default.iloc[k].split()\n",
        "    npal = 0\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      palabra_default = ' '\n",
        "      while palabras_d[npal] == '?' or palabras_d[npal] == '¿' or palabras_d[npal] == '.' or palabras_d[npal] == ',':\n",
        "        if palabras_d[npal] == '¿':\n",
        "          inicio_pregunta = True\n",
        "        npal += 1\n",
        "\n",
        "      if palabras_d[npal] != '?' and palabras_d[npal] != '¿' and palabras_d[npal] != '.' and palabras_d[npal] != ',':\n",
        "        palabra_default = palabras_d[npal]\n",
        "\n",
        "      if palabra_default[0] == \"¿\":\n",
        "        inicio_pregunta = True\n",
        "\n",
        "      if palabra_default[-1] == \"?\":\n",
        "        puntuacion_final = 3\n",
        "      elif palabra_default[-1] == \".\":\n",
        "        puntuacion_final = 1\n",
        "      elif palabra_default[-1] == \",\":\n",
        "        puntuacion_final = 2\n",
        "      else:\n",
        "        if npal != len(palabras_d)-1:\n",
        "          if palabras_d[npal+1] == '?':\n",
        "            puntuacion_final = 3\n",
        "          elif palabras_d[npal+1] == '.':\n",
        "            puntuacion_final = 1\n",
        "          elif palabras_d[npal+1] == ',':\n",
        "            puntuacion_final = 2\n",
        "          else:\n",
        "            puntuacion_final = 0\n",
        "        else:\n",
        "          puntuacion_final = 0\n",
        "\n",
        "      #categoria = indice_categoria_stanza(palabra)\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      if palabra_default.islower():\n",
        "        capitalizacion = 0\n",
        "      elif palabra_default.istitle():\n",
        "        capitalizacion = 1\n",
        "      elif palabra_default.isupper():\n",
        "        capitalizacion = 3\n",
        "      else:\n",
        "        capitalizacion = 2\n",
        "\n",
        "      npal += 1\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria) #cambiar\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuaciones_iniciales.append(1)\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuaciones_iniciales.append(0)\n",
        "\n",
        "        if j == n_tok - 1:\n",
        "          puntuaciones_finales.append(puntuacion_final)\n",
        "        else:\n",
        "          puntuaciones_finales.append(0)\n",
        "\n",
        "        capitalizaciones.append(capitalizacion)\n",
        "        instancia_ids.append(instancia_id)\n",
        "        palabras_default.append(palabra_default)\n",
        "\n",
        "  data_set_RF_con_etiquetas['palabra_default'] = palabras_default\n",
        "  data_set_RF_con_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_con_etiquetas['token'] = tokens\n",
        "  data_set_RF_con_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_con_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_con_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_con_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_con_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_con_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_con_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_con_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_con_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_con_etiquetas['forma_parte'] = forman_parte\n",
        "  data_set_RF_con_etiquetas['punt_inicial'] = puntuaciones_iniciales\n",
        "  data_set_RF_con_etiquetas['punt_final'] = puntuaciones_finales\n",
        "  data_set_RF_con_etiquetas['capitalización'] = capitalizaciones\n",
        "\n",
        "  return data_set_RF_con_etiquetas"
      ],
      "metadata": {
        "id": "kdihneKxf8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos y creación de data set"
      ],
      "metadata": {
        "id": "Bi10oO7bd2wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'\n",
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'\n",
        "path = path + libro1"
      ],
      "metadata": {
        "id": "v6C-6HTBsg24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2WkOAYegsoBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos = convertir_epub_a_pd('libro1.epub')"
      ],
      "metadata": {
        "id": "n4DpCsmosqjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF_con_etiquetas = crearDataSetRFConEtiquetas(parrafos)\n",
        "dataSet_RF_sin_etiquetas = crearDataSetRFSinEtiquetas(parrafos['limpio'])"
      ],
      "metadata": {
        "id": "ywTmyS5UtJg8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF_con_etiquetas"
      ],
      "metadata": {
        "id": "sFt72T1-gDeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF_sin_etiquetas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nRHVQCIQgHAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 1: capitalización\n",
        "* 0: todo en minúsculas (ej: “hola”)\n",
        "* 1: primera letra en mayúscula (ej: “Hola”) (incluye palabras de 1 letra)\n",
        "* 2: algunas (pero no todas) letras en mayúscula (ej: “McDonald's”  “iPhone”)\n",
        "* 3: todo en mayúsculas (ej.: “ONU”, “NASA”, “UBA”) (más de una letra)"
      ],
      "metadata": {
        "id": "Ehs4pzDL1miS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 2: puntuación inicial\n",
        "* 0: sin puntuación.\n",
        "* 1: \"?\""
      ],
      "metadata": {
        "id": "FSWw3oCTgdig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 3: puntuación final\n",
        "* 0: sin puntuación.\n",
        "* 1: \".\"\n",
        "* 2: \",\"\n",
        "* 3: \"?\""
      ],
      "metadata": {
        "id": "VGVr6l9gguUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separación en test y train"
      ],
      "metadata": {
        "id": "tCuoqhFYhBZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataSet_RF_con_etiquetas[dataSet_RF_con_etiquetas['instancia_id'] <= 2972]\n",
        "test = dataSet_RF_con_etiquetas[dataSet_RF_con_etiquetas['instancia_id'] > 2972]"
      ],
      "metadata": {
        "id": "CYjUNCywwPpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train.drop(columns=['palabra_default', 'token', 'categoria_gramatical', 'capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_train = train[['capitalización', 'punt_inicial', 'punt_final']]\n",
        "X_test = test.drop(columns=['palabra_default', 'token', 'categoria_gramatical', 'capitalización', 'punt_inicial', 'punt_final'])\n",
        "y_test = test[['capitalización', 'punt_inicial', 'punt_final']]"
      ],
      "metadata": {
        "id": "YdSBC6C4ynNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest para etiqueta 1"
      ],
      "metadata": {
        "id": "ScpDtm8_omtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train1 = y_train['capitalización']\n",
        "y_test1 = y_test['capitalización']"
      ],
      "metadata": {
        "id": "RMa8CWBvzQgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train, y_train1)\n",
        "\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "id": "_SSIaJgB5wBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train, y_train1)'''"
      ],
      "metadata": {
        "id": "s-1g2bDCx3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = grid.predict(X_test)"
      ],
      "metadata": {
        "id": "CWF43puSz8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test1, y_pred1, average = \"macro\")"
      ],
      "metadata": {
        "id": "Bd0pipyx3ftv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test1,y_pred1, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "r1kf-Jt71ITx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest para etiqueta 2"
      ],
      "metadata": {
        "id": "KRFYt6XOhJIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train2 = y_train['punt_inicial']\n",
        "y_test2 = y_test['punt_inicial']"
      ],
      "metadata": {
        "id": "so4LY8NK2eh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train, y_train2)"
      ],
      "metadata": {
        "id": "QQQhKJ7q2kGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "QXW0Fid82oRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test2, y_pred2, average = \"macro\")"
      ],
      "metadata": {
        "id": "6EByF3Px2q0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test2, y_pred2, labels=[0,1])"
      ],
      "metadata": {
        "id": "8ewx5ROy2t_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest para etiqueta 3"
      ],
      "metadata": {
        "id": "c-YY-UO2hKT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train3 = y_train['punt_final']\n",
        "y_test3 = y_test['punt_final']"
      ],
      "metadata": {
        "id": "tvXCw98Y3jyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, min_samples_split=10)\n",
        "model.fit(X_train, y_train3)"
      ],
      "metadata": {
        "id": "HLMzxX8v3n5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred3 = model.predict(X_test)"
      ],
      "metadata": {
        "id": "yKRrST3o3rU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test3, y_pred3, average = \"macro\")"
      ],
      "metadata": {
        "id": "jN6H6km53tQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test3, y_pred3, labels=[0,1,2,3])"
      ],
      "metadata": {
        "id": "rGVFU5lL3vlB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}