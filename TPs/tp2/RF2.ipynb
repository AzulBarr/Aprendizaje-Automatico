{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "H78qTC9BlZMq",
        "AsBnofU0ldDm",
        "ULs3Rgzzd_04",
        "Ehs4pzDL1miS",
        "CZ8fuRwIp1hn",
        "55DaPLK8p5VD",
        "TSWNUrgdwMqI"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/TPs/tp2/RF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de librerias y modelos"
      ],
      "metadata": {
        "id": "H78qTC9BlZMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib beautifulsoup4 pandas\n",
        "!pip install stanza"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Wct7dMGqvR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ3TrxZflROM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ebooklib import epub\n",
        "import ebooklib\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import stanza\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RKHUgiJplYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"es\")\n",
        "nlp = stanza.Pipeline(\"es\", processors=\"tokenize,pos\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5lCZKV0Pei6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de funciones"
      ],
      "metadata": {
        "id": "AsBnofU0ldDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_X(t):\n",
        "    # Convertir a minúsculas y quitar puntuación\n",
        "    t = t.lower()\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-záéíóúüñ0-9' -]+\", ' ', t)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*e\\s*\\.?\\s*d\\s*\\.?\\s*d\\s*\\.?\\s*o\\s*\\.?', 'peddo', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r's\\s*\\.?\\s*p\\s*\\.?\\s*a\\s*\\.?\\s*d\\s*\\.?\\s*a\\.?', 'spada', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "abGSiELDq-AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text_y(t):\n",
        "    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n",
        "    t = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚ0-9¿?,.' -]+\", ' ', t)\n",
        "    t = re.sub(r'p\\s*\\.?\\s*e\\s*\\.?\\s*d\\s*\\.?\\s*d\\s*\\.?\\s*o\\s*\\.?', 'PEDDO', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r's\\s*\\.?\\s*p\\s*\\.?\\s*a\\s*\\.?\\s*d\\s*\\.?\\s*a\\.?', 'SPADA', t, flags=re.IGNORECASE)\n",
        "    t = re.sub(r'\\s+', ' ', t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "L41yJLZ-rFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertir_epub_a_pd(archivo_epub='libro.epub'):\n",
        "  # Cargar el libro\n",
        "  book = ebooklib.epub.read_epub(archivo_epub)\n",
        "\n",
        "  # Lista donde se guardarán los párrafos\n",
        "  parrafos = []\n",
        "\n",
        "  # Recorremos los ítems del libro\n",
        "  for item in book.get_items():\n",
        "      if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "          # Parseamos el contenido HTML\n",
        "          soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
        "          # Extraemos los párrafos\n",
        "          for p in soup.find_all('p'):\n",
        "            #print(\"p:\",p, 'tipo: ', type(p))\n",
        "            texto = p.get_text().strip()\n",
        "            #print(\"TEXTO:\",texto, ' tipo: ', type(texto))\n",
        "            palabras = texto.split()\n",
        "            #print(\"PALABRAS:\",palabras, ' tipo: ', type(palabras))\n",
        "            if len(palabras) < 20 or len(palabras) > 100:  # descartamos párrafos cortos\n",
        "                continue\n",
        "            if texto:\n",
        "                parrafos.append(texto)\n",
        "\n",
        "  df = pd.DataFrame({'parrafo': parrafos})\n",
        "  df.to_csv(\"libro_parrafos.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print(f\"Se extrajeron {len(parrafos)} párrafos y se guardaron en 'libro_parrafos.csv'.\")\n",
        "\n",
        "  f = pd.read_csv('libro_parrafos.csv')\n",
        "  parrafos = pd.DataFrame(columns=['default', 'limpio'])\n",
        "  parrafos['limpio'] = df['parrafo'].apply(normalize_text_X)\n",
        "  parrafos['default'] = df['parrafo'].apply(normalize_text_y)\n",
        "\n",
        "  return parrafos"
      ],
      "metadata": {
        "id": "y48N-5J8s7We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoria_gramatical_stanza(palabra):\n",
        "    doc = nlp(palabra)\n",
        "    token = doc.sentences[0].words[0]\n",
        "    return token.upos\n",
        "\n",
        "upos2id = {\n",
        "    \"NOUN\": 0, #Sustantivo común. Ej: gato, casa, libro, profesor\n",
        "    \"PROPN\": 1, #Sustantivo propio. Ej: Argentina, Azul, Google\n",
        "    \"VERB\": 2, #Verbo léxico. Ej: comer, hablar, correr\n",
        "    \"ADJ\": 3, #Adjetivo. Ej: rápido, azul, brillante\n",
        "    \"ADV\": 4, #Adverbio. Ej: rápidamente, muy, cerca\n",
        "    \"PRON\": 5, #Pronombre. Ej: yo, tú, él, eso, alguien\n",
        "    \"DET\": 6, #Determinante / artículo. Ej: el, la, los, un, ese, mi\n",
        "    \"ADP\": 7, #Adposición: preposición o posposición. Ej: de, para, con, sin, sobre\n",
        "    \"SCONJ\": 8, #Conjunción subordinante. Ej: que, porque, aunque, si\n",
        "    \"CCONJ\": 9, #Conjunción coordinante. Ej: y, o, pero, ni\n",
        "    \"NUM\": 10, #Numeral. Ej: uno, dos, 50, tercero\n",
        "    \"INTJ\": 11, #Interjección. Ej: ay!, hola!, uf, eh\n",
        "    \"PART\": 12, #Partícula gramatical (raro en español). Ejemplos típicos en inglés (not, 's), en español casi no se usa, pero aparece en casos como \"sí\" enfático.\n",
        "    \"AUX\": 13, #Verbo auxiliar. Ej: haber, ser (cuando forman tiempos compuestos: “he comido”, “está hablando”)\n",
        "    \"PUNCT\": 14, #Signos de puntuación. Ej: , . ; ! ?\n",
        "    \"SYM\": 15, #Símbolos. Ej: $, %, +, =, →\n",
        "    \"X\": 16 #Otros / desconocidos / extranjeros. Cualquier cosa que no encaja en ninguna categoría.\n",
        "}\n",
        "\n",
        "def indice_categoria_stanza(palabra):\n",
        "    pos = categoria_gramatical_stanza(palabra)\n",
        "    return upos2id.get(pos,-1)"
      ],
      "metadata": {
        "id": "yXHgA4Nke0dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFSinEtiquetas(parrafo):\n",
        "  data_set_RF_sin_etiquetas = pd.DataFrame(columns = ['instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte'])\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "\n",
        "  for k, parrafo in enumerate(parrafo):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      #categoria = indice_categoria_stanza(palabra)\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria) #cambiar\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        instancia_ids.append(instancia_id)\n",
        "\n",
        "  data_set_RF_sin_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_sin_etiquetas['token'] = tokens\n",
        "  data_set_RF_sin_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_sin_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_sin_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_sin_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_sin_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_sin_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_sin_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_sin_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_sin_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_sin_etiquetas['forma_parte'] = forman_parte\n",
        "\n",
        "  return data_set_RF_sin_etiquetas"
      ],
      "metadata": {
        "id": "saVcamHuERO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crearDataSetRFConEtiquetas(parrafos):\n",
        "  data_set_RF_con_etiquetas = pd.DataFrame(columns = ['palabra_default', 'instancia_id', 'token', 'token_id', 'posicion_frase',\n",
        "                                        'categoria_gramatical', 'distancia_al_final',\n",
        "                                        'id_anterior', 'id_siguiente', 'es_principio',\n",
        "                                        'es_medio', 'es_final', 'forma_parte', 'punt_inicial', 'punt_final', 'capitalización'])\n",
        "\n",
        "  palabras_default = []\n",
        "\n",
        "  instancia_ids = []\n",
        "  tokens = []\n",
        "  token_ids = []\n",
        "  posiciones_parrafo = []\n",
        "  categorias_gramaticales = []\n",
        "  distancias_al_final = []\n",
        "  ids_anteriores = []\n",
        "  ids_siguientes = []\n",
        "  son_principio = []\n",
        "  son_medio = []\n",
        "  son_final = []\n",
        "  forman_parte = []\n",
        "  puntuaciones_iniciales = []\n",
        "  puntuaciones_finales = []\n",
        "  capitalizaciones = []\n",
        "\n",
        "  datos_limpios = parrafos['limpio']\n",
        "  datos_default = parrafos['default']\n",
        "  for k, parrafo in enumerate(datos_limpios):\n",
        "    instancia_id = k\n",
        "    token_siguiente = -1\n",
        "    token_anterior = -1\n",
        "    palabras = parrafo.split()\n",
        "\n",
        "    inicio_pregunta = False\n",
        "\n",
        "    palabras_d = datos_default.iloc[k].split()\n",
        "    npal = 0\n",
        "    for i, palabra in enumerate(palabras):\n",
        "      palabra_default = ' '\n",
        "      while palabras_d[npal] == '?' or palabras_d[npal] == '¿' or palabras_d[npal] == '.' or palabras_d[npal] == ',':\n",
        "        if palabras_d[npal] == '¿':\n",
        "          inicio_pregunta = True\n",
        "        npal += 1\n",
        "\n",
        "      if palabras_d[npal] != '?' and palabras_d[npal] != '¿' and palabras_d[npal] != '.' and palabras_d[npal] != ',':\n",
        "        palabra_default = palabras_d[npal]\n",
        "\n",
        "      if palabra_default[0] == \"¿\":\n",
        "        inicio_pregunta = True\n",
        "\n",
        "      if palabra_default[-1] == \"?\":\n",
        "        puntuacion_final = 3\n",
        "      elif palabra_default[-1] == \".\":\n",
        "        puntuacion_final = 1\n",
        "      elif palabra_default[-1] == \",\":\n",
        "        puntuacion_final = 2\n",
        "      else:\n",
        "        if npal != len(palabras_d)-1:\n",
        "          if palabras_d[npal+1] == '?':\n",
        "            puntuacion_final = 3\n",
        "          elif palabras_d[npal+1] == '.':\n",
        "            puntuacion_final = 1\n",
        "          elif palabras_d[npal+1] == ',':\n",
        "            puntuacion_final = 2\n",
        "          else:\n",
        "            puntuacion_final = 0\n",
        "        else:\n",
        "          puntuacion_final = 0\n",
        "\n",
        "      #categoria = indice_categoria_stanza(palabra)\n",
        "      categoria = 0\n",
        "      tokens_de_palabra = tokenizer.tokenize(palabra)\n",
        "\n",
        "      if palabra_default.islower():\n",
        "        capitalizacion = 0\n",
        "      elif palabra_default.istitle():\n",
        "        capitalizacion = 1\n",
        "      elif palabra_default.isupper():\n",
        "        capitalizacion = 3\n",
        "      else:\n",
        "        capitalizacion = 2\n",
        "\n",
        "      npal += 1\n",
        "      for j, token in enumerate(tokens_de_palabra):\n",
        "        id = tokenizer.convert_tokens_to_ids(token)\n",
        "        tokens.append(token)\n",
        "        token_ids.append(id)\n",
        "        posiciones_parrafo.append(i)\n",
        "        categorias_gramaticales.append(categoria) #cambiar\n",
        "        distancias_al_final.append(len(palabras) - i)\n",
        "\n",
        "        ids_anteriores.append(token_anterior)\n",
        "        token_anterior = id\n",
        "\n",
        "        n_tok = len(tokens_de_palabra)\n",
        "        if j != n_tok - 1:\n",
        "          id_sig  = tokenizer.convert_tokens_to_ids(tokens_de_palabra[j + 1])\n",
        "          ids_siguientes.append(id_sig)\n",
        "        else:\n",
        "          if i != len(palabras) - 1:\n",
        "            token_siguiente = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(palabras[i + 1]))[0]\n",
        "            ids_siguientes.append(token_siguiente)\n",
        "          else:\n",
        "            ids_siguientes.append(-1)\n",
        "\n",
        "        es_medio_id = 0\n",
        "        es_final_id = 0\n",
        "        es_principio_id = 0\n",
        "        if j == 0:\n",
        "          es_principio_id = 1\n",
        "        elif j == n_tok - 1:\n",
        "          es_final_id = 1\n",
        "        else:\n",
        "          es_medio_id = 1\n",
        "        son_principio.append(es_principio_id)\n",
        "        son_medio.append(es_medio_id)\n",
        "        son_final.append(es_final_id)\n",
        "\n",
        "        forma_parte_id = 0\n",
        "        if n_tok != 1:\n",
        "          forma_parte_id = 1\n",
        "        forman_parte.append(forma_parte_id)\n",
        "\n",
        "        if inicio_pregunta:\n",
        "          puntuaciones_iniciales.append(1)\n",
        "          inicio_pregunta = False\n",
        "        else:\n",
        "          puntuaciones_iniciales.append(0)\n",
        "\n",
        "        if j == n_tok - 1:\n",
        "          puntuaciones_finales.append(puntuacion_final)\n",
        "        else:\n",
        "          puntuaciones_finales.append(0)\n",
        "\n",
        "        capitalizaciones.append(capitalizacion)\n",
        "        instancia_ids.append(instancia_id)\n",
        "        palabras_default.append(palabra_default)\n",
        "\n",
        "  data_set_RF_con_etiquetas['palabra_default'] = palabras_default\n",
        "  data_set_RF_con_etiquetas['instancia_id'] = instancia_ids\n",
        "  data_set_RF_con_etiquetas['token'] = tokens\n",
        "  data_set_RF_con_etiquetas['token_id'] = token_ids\n",
        "  data_set_RF_con_etiquetas['posicion_frase'] = posiciones_parrafo\n",
        "  data_set_RF_con_etiquetas['categoria_gramatical'] = categorias_gramaticales\n",
        "  data_set_RF_con_etiquetas['distancia_al_final'] = distancias_al_final\n",
        "  data_set_RF_con_etiquetas['id_anterior'] = ids_anteriores\n",
        "  data_set_RF_con_etiquetas['id_siguiente'] = ids_siguientes\n",
        "  data_set_RF_con_etiquetas['es_principio'] = son_principio\n",
        "  data_set_RF_con_etiquetas['es_medio'] = son_medio\n",
        "  data_set_RF_con_etiquetas['es_final'] = son_final\n",
        "  data_set_RF_con_etiquetas['forma_parte'] = forman_parte\n",
        "  data_set_RF_con_etiquetas['punt_inicial'] = puntuaciones_iniciales\n",
        "  data_set_RF_con_etiquetas['punt_final'] = puntuaciones_finales\n",
        "  data_set_RF_con_etiquetas['capitalización'] = capitalizaciones\n",
        "\n",
        "  return data_set_RF_con_etiquetas"
      ],
      "metadata": {
        "id": "kdihneKxf8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos y creación de data set"
      ],
      "metadata": {
        "id": "Bi10oO7bd2wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/AzulBarr/Aprendizaje-Automatico/main/TPs/tp2'\n",
        "libro1 = '/Harry_Potter_y_el_caliz_de_fuego_J_K_Rowling.epub'\n",
        "path = path + libro1"
      ],
      "metadata": {
        "id": "v6C-6HTBsg24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O libro1.epub $path"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2WkOAYegsoBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parrafos = convertir_epub_a_pd('libro1.epub')"
      ],
      "metadata": {
        "id": "n4DpCsmosqjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF_con_etiquetas = crearDataSetRFConEtiquetas(parrafos)\n",
        "dataSet_RF_sin_etiquetas = crearDataSetRFSinEtiquetas(parrafos['limpio'])"
      ],
      "metadata": {
        "id": "ywTmyS5UtJg8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atributos para el data set"
      ],
      "metadata": {
        "id": "ULs3Rgzzd_04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF = crearDataSetRF(parrafos['limpio'])\n",
        "#dataSet_RF = pd.read_csv('dataSetRFSinEtiquetas.csv')"
      ],
      "metadata": {
        "id": "cTyP1SzBgopl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF.to_csv('dataSetRFSinEtiquetas.csv', index=False)"
      ],
      "metadata": {
        "id": "fB-mQQ6g5FEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etiqueta 1: capitalización\n",
        "* 0: todo en minúsculas (ej: “hola”)\n",
        "* 1: primera letra en mayúscula (ej: “Hola”) (incluye palabras de 1 letra)\n",
        "* 2: algunas (pero no todas) letras en mayúscula (ej: “McDonald's”  “iPhone”)\n",
        "* 3: todo en mayúsculas (ej.: “ONU”, “NASA”, “UBA”) (más de una letra)"
      ],
      "metadata": {
        "id": "Ehs4pzDL1miS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, c = dataSet_RF.shape\n",
        "f2, c2 = dataSet.shape\n",
        "#dataSet_RF['capitalizacion'] = dataSet[:f]['capitalización']"
      ],
      "metadata": {
        "id": "A_Jlzwh71LsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF[dataSet_RF['token_id'] == 10605]"
      ],
      "metadata": {
        "id": "9hC4TSMNgDYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet_RF[dataSet_RF['token_id'] == 28163]"
      ],
      "metadata": {
        "id": "PfYsY0jBgQ-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataSet[dataSet['token_id'] ==  30519]"
      ],
      "metadata": {
        "id": "he7PBAEE2Xwu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataSet['token_id'])\n",
        "print(dataSet_RF['token_id'])"
      ],
      "metadata": {
        "id": "v39Vjnds1fiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc74acb1"
      },
      "source": [
        "print(f\"Length of dataSet['token_id']: {len(dataSet['token_id'])}\")\n",
        "print(f\"Length of dataSet_RF['token_id']: {len(dataSet_RF['token_id'])}\")\n",
        "\n",
        "unique_tokens_dataSet = set(dataSet['token_id'].unique())\n",
        "unique_tokens_dataSet_RF = set(dataSet_RF['token_id'].unique())\n",
        "\n",
        "diff_only_in_dataSet = unique_tokens_dataSet - unique_tokens_dataSet_RF\n",
        "diff_only_in_dataSet_RF = unique_tokens_dataSet_RF - unique_tokens_dataSet\n",
        "\n",
        "if not diff_only_in_dataSet and not diff_only_in_dataSet_RF:\n",
        "    print(\"\\nBoth series contain the same unique token_ids, although their lengths might differ.\")\n",
        "else:\n",
        "    if diff_only_in_dataSet:\n",
        "        print(f\"\\nToken_ids present in dataSet but not in dataSet_RF (first 10): {list(diff_only_in_dataSet)[:10]}\")\n",
        "    if diff_only_in_dataSet_RF:\n",
        "        print(f\"\\nToken_ids present in dataSet_RF but not in dataSet (first 10): {list(diff_only_in_dataSet_RF)[:10]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "ScpDtm8_omtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_Y['instancia_id'] = datos_X['instancia_id']\n",
        "\n",
        "datos_X[\"instancia_id\"] = pd.to_numeric(datos_X[\"instancia_id\"], errors=\"coerce\")\n",
        "datos_Y[\"instancia_id\"] = pd.to_numeric(datos_Y[\"instancia_id\"], errors=\"coerce\")\n",
        "\n",
        "X_train = datos_X[datos_X['instancia_id'] < 2955]\n",
        "y_train = datos_Y[datos_Y['instancia_id'] < 2955]\n",
        "\n",
        "X_test = datos_X[datos_X['instancia_id'] >= 2955]\n",
        "y_test = datos_Y[datos_Y['instancia_id'] >= 2955]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LA-bKg-lS5pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['embeddings']"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rh1kNF7fygRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"embeddings_mean\"] = X_train[\"embeddings\"].apply(lambda x: np.mean(x))\n"
      ],
      "metadata": {
        "id": "qqsoPn2jzJL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[\"embeddings_mean\"] = X_test[\"embeddings\"].apply(lambda x: np.mean(x))\n"
      ],
      "metadata": {
        "id": "0lZ5s8JZz4yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "\n",
        "    #n_estimators cantidad de arboles\n",
        "    # max_depth altura maxima de cada uno\n"
      ],
      "metadata": {
        "id": "s-1g2bDCx3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "wF-CGyJn2_j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train[['embeddings_mean']], y_train.drop(columns=['instancia_id']))\n"
      ],
      "metadata": {
        "id": "GPgR1LkqyEDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test[['embeddings_mean']])\n"
      ],
      "metadata": {
        "id": "CWF43puSz8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test['punt_inicial'], y_pred[:,0],average=\"macro\")"
      ],
      "metadata": {
        "id": "y2KfWJUV0bbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test['punt_inicial'],y_pred[:,1],average = \"macro\")"
      ],
      "metadata": {
        "id": "sBu0bVfE3V8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test['capitalización'],y_pred[:,2],average = \"macro\")"
      ],
      "metadata": {
        "id": "Bd0pipyx3ftv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Unidireccional"
      ],
      "metadata": {
        "id": "CZ8fuRwIp1hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,#como es 2, significa que hay dos bloques de celdas LSTM\n",
        "            batch_first=True, #(batch, seq, feature)\n",
        "            dropout=dropout, #dropout probability\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)\n"
      ],
      "metadata": {
        "id": "aauZ2P-3oonB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderUnidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderUnidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=False  # unidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }"
      ],
      "metadata": {
        "id": "56o8-fDTpfbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder"
      ],
      "metadata": {
        "id": "EH-ISLOVv2Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloUnidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloUnidireccional, self).__init__()\n",
        "        self.encoder = EncoderUnidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderUnidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "3ZoHpllJpyRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN Bidireccional"
      ],
      "metadata": {
        "id": "55DaPLK8p5VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(EncoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        \"\"\"\n",
        "        embeddings: tensor de forma (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "        # outputs: (batch_size, seq_len, hidden_dim)\n",
        "        # hidden: (num_layers, batch_size, hidden_dim)\n",
        "        # cell:   (num_layers, batch_size, hidden_dim)\n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "vNB4cjjAp4_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBidireccional(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(DecoderBidireccional, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True  # bidireccional\n",
        "        )\n",
        "\n",
        "        # Capa feed-forward para cada problema\n",
        "        self.punt_inicial_ff = nn.Linear(hidden_dim, 2)\n",
        "        self.punt_final_ff = nn.Linear(hidden_dim, 4)\n",
        "        self.capital_ff = nn.Linear(hidden_dim, 4)\n",
        "\n",
        "        # Función de activación para cada problema\n",
        "        #self.punt_inicial_sigmoid = nn.Sigmoid()\n",
        "        #self.punt_final_softmax = nn.Softmax(dim=4)\n",
        "        #self.capital_softmax = nn.Softmax(dim=4)\n",
        "\n",
        "\n",
        "    def forward(self, encoder_outputs, hidden, cell):\n",
        "        \"\"\"\n",
        "        encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
        "        hidden, cell: del encoder\n",
        "        \"\"\"\n",
        "        outputs, _ = self.lstm(encoder_outputs, (hidden, cell))\n",
        "\n",
        "        #punt_inicial_logits = self.punt_inicial_sigmoid(self.punt_inicial_ff(outputs))\n",
        "        #punt_final_logits = self.punt_final_sofmax(self.punt_final_ff(outputs))\n",
        "        #capital_logits = self.capital_sofmax(self.capital_ff(outputs))\n",
        "\n",
        "        punt_inicial_logits = self.punt_inicial_ff(outputs)\n",
        "        punt_final_logits = self.punt_final_ff(outputs)\n",
        "        capital_logits = self.capital_ff(outputs)\n",
        "\n",
        "        return {\n",
        "            \"puntuación inicial\": punt_inicial_logits,\n",
        "            \"puntuación final\": punt_final_logits,\n",
        "            \"capitalización\": capital_logits,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "AmRq0CiWwBCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder - Decoder bidireccional"
      ],
      "metadata": {
        "id": "-9h7wEuTwHG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModeloBidireccional(nn.Module):\n",
        "    def __init__(self, embedding_dim=768, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super(ModeloBidireccional, self).__init__()\n",
        "        self.encoder = EncoderBidireccional(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = DecoderBidireccional(hidden_dim, num_layers, dropout)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embeddings)\n",
        "        predictions = self.decoder(encoder_outputs, hidden, cell)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "oadDYbuowGS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "TSWNUrgdwMqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModeloUnidireccional(embedding_dim=768, hidden_dim=256, num_layers=2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)  # ignorar padding\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for embeddings, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(embeddings)  # diccionario con tus tres salidas\n",
        "\n",
        "        loss_inicial = criterion(outputs[\"puntuación inicial\"].permute(0,2,1), labels[\"punt_inicial\"])\n",
        "        loss_final = criterion(outputs[\"puntuación final\"].permute(0,2,1), labels[\"punt_final\"])\n",
        "        loss_cap = criterion(outputs[\"capitalización\"].permute(0,2,1), labels[\"capitalizacion\"])\n",
        "\n",
        "        loss = loss_inicial + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "DfkgZ-QvwQFx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}