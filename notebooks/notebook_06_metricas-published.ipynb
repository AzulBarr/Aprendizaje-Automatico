{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/notebooks/notebook_06_metricas-published.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCLrUsc3lEay"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_06_metricas-published.ipynb)\n",
        "\n",
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3tzqZ6UmlEa4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g2OnFDWlEa6"
      },
      "source": [
        "## Matrices de confusión\n",
        "\n",
        "Recordemos que una matriz de confusión nos permite observar el resultado de nuestra clasificación. Para armarla:\n",
        "- cada fila los valores observados o reales\n",
        "- cada columna los valores predichos\n",
        "\n",
        "Y en cada celda:\n",
        "- $m_{i,i}$ las instancias bien clasificadas\n",
        "- $m_{i,j}\\ (con\\ i\\neq j)$ las instancias mal clasificadas (era de instancia $i$ pero el clasificador dijo $j$)\n",
        "\n",
        "Implementar la siguiente función para poder construir una matriz de confusión binaria. Deberá tomar la etiqueta que es considerada \"éxito\" como parámetro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cKIFjfjlEa7"
      },
      "source": [
        "from typing import Tuple, Any\n",
        "\n",
        "def confusion_matrix(y_real: list, y_predicted: list, positive_label: Any, show: bool =False) -> Tuple[int, int, int, int]:\n",
        "    # Construye una matriz de confusión (binaria)\n",
        "    # y_actual es la secuencia de etiquetas reales\n",
        "    # y_predicted es la secuencia de etiquetas predichas por el clasificador\n",
        "    # positive_label indica cuál es la etiqueta considerada positiva.\n",
        "\n",
        "    tp = (y_real == positive_label) & (y_real == y_predicted)\n",
        "    tp = tp.sum()  # verdaderos positivos\n",
        "    tn = (y_real != positive_label) & (y_real == y_predicted)  # verdaderos negativos\n",
        "    tn = tn.sum()\n",
        "    fp = (y_predicted == positive_label) & (y_real != y_predicted)  # falsos positivos\n",
        "    fp = fp.sum()\n",
        "    fn = (y_predicted != positive_label) & (y_real != y_predicted)  # falsos negativos\n",
        "    fn = fn.sum()\n",
        "\n",
        "    if show:\n",
        "        display(pd.DataFrame([[tp, fn], [fp, tn]], index=[\"real +\", \"real -\"], columns=[\"pred +\", \"pred -\"]))\n",
        "\n",
        "\n",
        "    return tp, tn, fp, fn\n"
      ],
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLIf3lgrlEa8"
      },
      "source": [
        "### Test 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZEvgMsdlEa9"
      },
      "source": [
        "Vamos a probar ahora la función. Sabiendo que se recibieron 990 emails. Y que se recibieron, en este orden:\n",
        "  - 10 correos no deseados\n",
        "  - 978 correos\n",
        "  - 2 correos no deseados\n",
        "  \n",
        "El filtro anti-spam estableció las siguientes clasificaciones (también en órden):\n",
        "  - 2 correos no deseados\n",
        "  - 900 correos\n",
        "  - 20 correos no deseados\n",
        "  - 68 correos\n",
        "  \n",
        "Construir dos listas de strings que contengan `\"spam\"` o `\"no-spam\"` y que representen la etiqueta real (`y_real`) y la etiqueta predicha por el filtro anti-spam (`y_pred`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcmzGRxilEa9"
      },
      "source": [
        "y_real = np.concatenate((np.repeat(\"spam\", 10), np.repeat(\"no-spam\", 978), np.repeat(\"spam\", 2)))\n",
        "y_pred = np.concatenate((np.repeat(\"spam\", 2), np.repeat(\"no-spam\", 900), np.repeat(\"spam\", 20), np.repeat(\"no-spam\", 68)))"
      ],
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOQ6lUAElEa-"
      },
      "source": [
        "Correr la matriz de confusión y verificar que el resultado es el esperado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s5C_BxzvlEa_",
        "outputId": "e5135db9-d287-41ce-cee5-652fcb760343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1\n",
            "(tp, tn, fp, fn) =  (np.int64(2), np.int64(958), np.int64(20), np.int64(10))\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred, positive_label=\"spam\", show=False)\n",
        "# Si se cambia show a True se puede visualizar la matriz de confusión\n",
        "\n",
        "print(\"Test 1\")\n",
        "print(\"(tp, tn, fp, fn) = \", (tp, tn, fp, fn))\n",
        "assert((tp, tn, fp, fn) == (2, 958, 20, 10))\n",
        "print(\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcUU5-A9lEa_"
      },
      "source": [
        "## Métricas\n",
        "\n",
        "En esta sección trabajeremos con las métricas estándares de clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPl7_CmhlEbA"
      },
      "source": [
        "### Test 2\n",
        "A continuacion completar las funciones que computan las distintas métricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ3P9s4VlEbA"
      },
      "source": [
        "def accuracy_score(tp: int, tn: int, fp: int, fn: int) -> float:\n",
        "    return (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "\n",
        "def precision_score(tp: int, tn: int, fp: int, fn: int) -> float:\n",
        "    return tp / (tp + fp)\n",
        "\n",
        "\n",
        "def recall_score(tp: int, tn: int, fp: int, fn: int) -> float:\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "\n",
        "def f_beta_score(tp: int, tn: int, fp: int, fn: int, beta: float) -> float:\n",
        "    prec = precision_score(tp, tn, fp, fn)\n",
        "    recl = recall_score(tp, tn, fp, fn)\n",
        "    return (1 + beta**2) * (prec * recl) / (beta**2 * prec + recl)\n",
        "\n",
        "\n",
        "def f1_score(tp: int, tn: int, fp: int, fn: int) -> float:\n",
        "    return f_beta_score(tp, tn, fp, fn, beta=1)\n",
        "\n",
        "\n",
        "def all_metrics(tp: int, tn: int, fp: int, fn: int) -> float:\n",
        "    accuracy = round(accuracy_score(tp, tn, fp, fn), 3)\n",
        "    precision = round(precision_score(tp, tn, fp, fn), 3)\n",
        "    recall = round(recall_score(tp, tn, fp, fn), 3)\n",
        "    f1 = round(f1_score(tp, tn, fp, fn), 3)\n",
        "    return accuracy, precision, recall, f1\n"
      ],
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBIue-L1lEbB"
      },
      "source": [
        "Evaluar las funciones con el siguiente caso de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TtKDllP9lEbB",
        "outputId": "bc988ddb-1942-47fd-cdbe-e7ba10f18750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 2\n",
            "(accuracy, precision, recall, f1) =  (np.float64(0.97), np.float64(0.091), np.float64(0.167), np.float64(0.118))\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred, positive_label=\"spam\")\n",
        "(accuracy, precision, recall, f1) = all_metrics(tp, tn, fp, fn)\n",
        "\n",
        "print(\"Test 2\")\n",
        "print(\"(accuracy, precision, recall, f1) = \", (accuracy, precision, recall, f1))\n",
        "assert((accuracy, precision, recall, f1) == (0.97, 0.091, 0.167, 0.118))\n",
        "print(\"OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8AZeu-JlEbB"
      },
      "source": [
        "## Comparando predicciones\n",
        "\n",
        "Sean los siguientes datos provenientes de 2 clasificadores (A y B) y el valor real de las etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uCvIvc8RlEbC",
        "outputId": "9d4197c8-962b-4e87-dfec-cb9cf16ea28b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clasificador A, etiqueta de éxito: gato\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'bool' object has no attribute 'sum'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1610806310.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clasificador A, etiqueta de éxito: gato\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gato\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3863613120.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_real, y_predicted, positive_label, show)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpositive_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# verdaderos positivos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpositive_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# verdaderos negativos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'sum'"
          ]
        }
      ],
      "source": [
        "# Etiquetas reales\n",
        "y_real = [\"perro\"] * 18 + [\"gato\"] * 980 + [\"perro\"] * 5\n",
        "\n",
        "# Etiquetas devueltas por \"clasificador A\"\n",
        "y_pred_A = [\"gato\"] * 980 + [\"perro\"] * 20 + [\"gato\"] * 3\n",
        "\n",
        "# Etiquetas devueltas por \"clasificador B\"\n",
        "y_pred_B = [\"perro\"] * 40 + [\"gato\"] * 900 + [\"perro\"] * 60 + [\"gato\"] * 3\n",
        "\n",
        "df = pd.DataFrame(data={\"y_real\": y_real,\n",
        "                           \"y_pred_A\": y_pred_A,\n",
        "                           \"y_pred_B\": y_pred_B,\n",
        "                          })\n",
        "\n",
        "res = []\n",
        "print(\"Clasificador A, etiqueta de éxito: gato\")\n",
        "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_A, positive_label=\"gato\", show=True)\n",
        "res.append(all_metrics(tp, tn, fp, fn))\n",
        "\n",
        "print(\"Clasificador B, etiqueta de éxito: gato\")\n",
        "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_B, positive_label=\"gato\", show=True)\n",
        "res.append(all_metrics(tp, tn, fp, fn))\n",
        "\n",
        "print(\"Clasificador A, etiqueta de éxito: perro\")\n",
        "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_A, positive_label=\"perro\", show=True)\n",
        "res.append(all_metrics(tp, tn, fp, fn))\n",
        "\n",
        "print(\"Clasificador B, etiqueta de éxito: perro\")\n",
        "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_B, positive_label=\"perro\", show=True)\n",
        "res.append(all_metrics(tp, tn, fp, fn))\n",
        "\n",
        "pd.DataFrame(res, columns=[\"accuracy\", \"precision\", \"recall\", \"f1\"], index=[\"CLF A (gato)\", \"CLF B (gato)\", \"CLF A (perro)\", \"CLF B (gato)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZObfn6fNlEbC"
      },
      "source": [
        "¿Qué podemos concluir con este experimento?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxG1BEClEbC"
      },
      "source": [
        "## Analizando $F_1$\n",
        "\n",
        "A continuación realizamos un experimento variando levemente las condiciones en cada pasada.\n",
        "\n",
        "El código que realiza el experimento es el siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLex_maPlEbC"
      },
      "outputs": [],
      "source": [
        "y_real = [\"perro\"] * 100 + [\"gato\"] * 900 + [\"perro\"] * 80\n",
        "y_pred =   [\"perro\"] * 80 + [\"gato\"] * 800 + [\"perro\"] * 200\n",
        "\n",
        "tns_gato = []\n",
        "f1s_gato = []\n",
        "f1s_perro = []\n",
        "f1s_avg = []\n",
        "\n",
        "\n",
        "for i in range(0, 10000, 100):\n",
        "    y_real_2 = y_real + [\"perro\"] * i\n",
        "    y_pred_2 = y_pred + [\"perro\"] * i\n",
        "\n",
        "    tp1, tn1, fp1, fn1 = confusion_matrix(y_real=y_real_2, y_predicted=y_pred_2, positive_label=\"gato\")\n",
        "    tp2, tn2, fp2, fn2 = confusion_matrix(y_real=y_real_2, y_predicted=y_pred_2, positive_label=\"perro\")\n",
        "\n",
        "    f1_gato = f1_score(tp1, tn1, fp1, fn1)\n",
        "    f1_perro = f1_score(tp2, tn2, fp2, fn2)\n",
        "    f1_avg = (f1_gato + f1_perro) / 2\n",
        "\n",
        "    tns_gato.append(tn1)\n",
        "    f1s_gato.append(f1_gato)\n",
        "    f1s_perro.append(f1_perro)\n",
        "    f1s_avg.append(f1_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj0hcPCElEbD"
      },
      "source": [
        "1. ¿Qué realiza este experimento?\n",
        "1. ¿Qué relación existe entre la $F_1$ de perro y de gato a medida que se aumenta la cantidad de perros que tiene la muestra?\n",
        "1. ¿En algún punto valen lo mismo?¿En cuál?¿Por qué?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usk7CV_flEbD"
      },
      "source": [
        "A continuación se propone graficar cómo varía la métrica $F_1$ al aumentar la cantidad de True Negatives (observar\n",
        "que estamos cambiando la cantidad de instancias sobre las que testeamos).\n",
        "\n",
        "1. ¿Qué curva modifica más el agregado de las etiquetas `perro`?¿Por qué?\n",
        "1. ¿Qué se puede concluir de este experimento?\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuLJLnTWlEbD"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.plot(tns_gato, f1s_gato, \"*-\", label=\"F1_gato\")\n",
        "plt.plot(tns_gato, f1s_perro, \"o-\", label=\"F1_perro\")\n",
        "plt.plot(tns_gato, f1s_avg, \"x-\", label=\"F1_avg\")\n",
        "plt.xlabel(\"True Negatives (Gato)\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.ylim([0.5,1])\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}