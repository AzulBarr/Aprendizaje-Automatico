{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzulBarr/Aprendizaje-Automatico/blob/main/notebooks/notebook_09_redes_neuronales-published.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iBWv0_SfEXg"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_09_redes_neuronales-published.ipynb)\n",
        "\n",
        "# Redes neuronales\n",
        "\n",
        "\n",
        "Vamos nuevamente a trabajar con los datos de `iris` para entrenar (y antes construir) una Red Neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N7hVWBUfEXi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_data():\n",
        "    dataset = load_iris()\n",
        "    X = dataset[\"data\"]\n",
        "    y = dataset[\"target\"]\n",
        "    y = LabelEncoder().fit_transform(y)\n",
        "    return np.array(X), np.array(y)\n",
        "X, y = get_data()\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0b9qWI5fEXj"
      },
      "source": [
        "La propuesta es empezar por el esqueleto de las 2 clases que usaremos para esta tarea e ir implementado los m√©todos a medida que avancemos.\n",
        "\n",
        "Al final de este notebook se encuentran ambas clases completas. Pueden copiar el c√≥digo desde all√≠ mismo o implementarlo. La idea es que en cada avance podamos comprender la parte del proceso que estamos realizando, por lo cual se recomienda seguir la guia propuesta e ir completando s√≥lo lo que es necesario para cada punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2NWZcgkfEXk"
      },
      "outputs": [],
      "source": [
        "class Capa:\n",
        "    def __init__(self, neuronas):\n",
        "        self.neuronas = neuronas\n",
        "\n",
        "    def forward(self, inputs, weights, bias, activation):\n",
        "        \"\"\"\n",
        "        Forward Propagation de la capa\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def relu(self, inputs):\n",
        "        \"\"\"\n",
        "        ReLU: funci√≥n de activaci√≥n\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def softmax(self, inputs):\n",
        "        \"\"\"\n",
        "        Softmax: funci√≥n de activaci√≥n\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
        "        \"\"\"\n",
        "        Backward Propagation de la capa\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def relu_derivative(self, dA, Z):\n",
        "        \"\"\"\n",
        "        ReLU: gradiente de ReLU\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHCfg-uGfEXl"
      },
      "outputs": [],
      "source": [
        "class RedNeuronal:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.red = [] ## capas\n",
        "        self.arquitectura = [] ## mapeo de entradas -> salidas\n",
        "        self.pesos = [] ## W, b\n",
        "        self.memoria = [] ## Z, A\n",
        "        self.gradientes = [] ## dW, db\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def add(self, capa):\n",
        "        \"\"\"\n",
        "        Agregar capa a la red\n",
        "        \"\"\"\n",
        "        self.network.append(capa)\n",
        "\n",
        "    def _compile(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar la arquitectura\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _init_weights(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar arquitectura y los pesos\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _forwardprop(self, data):\n",
        "        \"\"\"\n",
        "        Pasada forward completa por la red\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _backprop(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Pasada backward completa por la red\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _update(self):\n",
        "        \"\"\"\n",
        "        Actualizar el modelo --> lr * gradiente\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _get_accuracy(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calcular accuracy despu√©s de cada iteraci√≥n\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _calculate_loss(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calcular cross-entropy loss despu√©s de cada iteraci√≥n\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train(self, X_train, y_train, epochs):\n",
        "        \"\"\"\n",
        "        Entrenar el modelo Stochastic Gradient Descent\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReeSc_CMfEXl"
      },
      "source": [
        "Los items que se presentan a continuaci√≥n tienen como objetivo explorar las clases que componen la red neuronal propuesta, comprender su arquitectura y funcionamiento.\n",
        "\n",
        "Nuevamente, lo ideal es no mirar todos los m√©todos hasta que llegue el momento de utilizarlos.\n",
        "\n",
        "1. Crear una Red Neuronal con 6 nodos en la primera capa, 8 en la segunda, 10 en la tercer y finalmente 3 en la √∫ltima, utilizando los m√©todos `add()`, `_compile()` de la clase `RedNeuronal` y el constructor de la clase `Capa`.\n",
        "  \n",
        "    Imprimir la arquitectura del modelo y asegurarse de obtener:\n",
        "\n",
        "    ```\n",
        "    [{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},\n",
        "    {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},\n",
        "    {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},\n",
        "    {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]\n",
        "    ```\n",
        "\n",
        "    Dibujar la red en papel.\n",
        "\n",
        "1. Inicializar los pesos de la red del punto anterior (`_init_weights(datos)`) y verificar que los pesos tienen dimensi√≥n correcta:\n",
        "\n",
        "    ```\n",
        "    capa 0: w=(4, 6) - b=(1, 6)\n",
        "    capa 1: w=(6, 8) - b=(1, 8)\n",
        "    capa 2: w=(8, 10) - b=(1, 10)\n",
        "    capa 3: w=(10, 3) - b=(1, 3)\n",
        "    ```\n",
        "\n",
        "    Definir las matrices que se corresponden con las capas de manera que una pasada pueda ser interpretada como el producto de todas ellas. Recordar que en cada paso por cada capa estaremos computando por cada neurona de la capa siguiente:\n",
        "\n",
        "    $$Z = \\sum_{i=1}^{n} X_i \\times W_i + b$$\n",
        "\n",
        "1. Funciones de activaci√≥n de una `Capa`:\n",
        "\n",
        "    1. Verificar que el funcionamiento de `ReLU` se corresponda con:\n",
        "\n",
        "        ```\n",
        "        if input > 0:\n",
        "            return input\n",
        "        else:\n",
        "            return 0\n",
        "        ```\n",
        "\n",
        "    1. Verificar que el funcionamiento de `softmax` se corresponda con:\n",
        "\n",
        "        $$\\sigma(Z)_i = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_j}}$$\n",
        "\n",
        "    **Nota**: para probar estos dos m√©todos puede ser util construir un vector de la siguiente manera: `np.array([[1.3, 5.1, -2.2, 0.7, 1.1]])` que genera un vector de tama√±o (1,5).\n",
        "\n",
        "1. Avancemos con `_forwardprop(datos)`, si corremos la red inicializada con los datos:\n",
        "\n",
        "    1. ¬øQu√© nos tipo de objeto nos devuelve este m√©todo?\n",
        "\n",
        "    1. ¬øQu√© quiere decir cada uno de los valores?\n",
        "\n",
        "    1. La primera fila, que se corresponder√≠a con la primera observaci√≥n del dataset, ¬øqu√© resultados nos da?¬øqu√© es m√°s probable: 'setosa', 'versicolor' o 'virginica'?¬øqu√© valor es el real?¬øpor qu√©?\n",
        "\n",
        "1. Arrancamos a propagar para atr√°s lo aprendido en la primera pasada. Esto lo realizaremos con el m√©todo `_backprop`.\n",
        "\n",
        "    1. ¬øC√≥mo es la derivada de la funci√≥n de activaci√≥n `ReLU`?¬øSu c√≥digo es correcto?\n",
        "\n",
        "    1. ¬øCu√°l es la operaci√≥n matem√°tica que hace la funci√≥n `backward` de la clase `Capa` en el caso de tener como activaci√≥n a `relu`?\n",
        "\n",
        "    1. El m√©todo `_backprop` toma 2 par√°metros: `predicted` y `actual`. ¬øqu√© debemos pasarle en dicho lugar?\n",
        "\n",
        "        Si la respuesta no fue: en `predicted` le pasamos el resultado de `_forwardprop(...)` y en `actual` le pasamos `y`.... volver a pensarlo. ;-)\n",
        "\n",
        "    1. Verificar que los `gradientes` y los `pesos` para cada una de las capas tienen el mismo tama√±o.\n",
        "\n",
        "1. Preparemos por √∫ltimo las funciones necesarias para el entrenamiento. Describir brevemente qu√© hacen las funciones:\n",
        "\n",
        "    - `_get_accuracy`\n",
        "    - `_calculate_loss`\n",
        "    - `_update`\n",
        "\n",
        "1. Incluyamos finalmente la funci√≥n `train` y entrenemos una red con la arquitectura propuesta en el punto 1 por 200 epocas.\n",
        "\n",
        "    1. ¬øQu√© valores se imprimen?¬øQu√© es posible interpretar de ellos?\n",
        "\n",
        "    1. Graficar el _accuracy_ y la _loss_ que arroja el entramiento en funci√≥n de las _epochs_. ¬øQu√© se puede concluir? Probablemente la se√±al sea ruidosa, por lo que se recomienda hacer un suavizado por ventanas deslizantes.\n",
        "\n",
        "1. Reimplementar la clase `RedNeuronal` utilizando PyTorch\n",
        "\n",
        "    Hasta ahora hemos construido nuestra propia red neuronal \"desde cero\", lo cual nos permiti√≥ comprender en profundidad c√≥mo funciona cada componente: inicializaci√≥n de pesos, funciones de activaci√≥n, forward y backward propagation, c√°lculo de loss y accuracy, y actualizaci√≥n de pesos.\n",
        "\n",
        "    Sin embargo, en proyectos reales y m√°s complejos, utilizamos frameworks como **PyTorch** que abstraen estas tareas, permiti√©ndonos enfocarnos m√°s en el dise√±o de la arquitectura y el an√°lisis de los resultados.  \n",
        "\n",
        "    **Objetivo de este inciso**: recrear la arquitectura y entrenamiento de nuestra red neuronal, pero usando herramientas provistas por PyTorch. Esto implica:\n",
        "\n",
        "    1. Implementar una clase `RedNeuronalTorch` que herede de `nn.Module` y contenga una red con la misma arquitectura:  \n",
        "    - Entrada de dimensi√≥n 4 (por las caracter√≠sticas del dataset Iris)\n",
        "    - Capas ocultas de 6, 8 y 10 nodos respectivamente\n",
        "    - Capa de salida con 3 nodos y activaci√≥n `softmax`\n",
        "\n",
        "    2. Entrenar esta nueva red por 200 √©pocas utilizando:\n",
        "    - Funci√≥n de p√©rdida: `nn.CrossEntropyLoss`\n",
        "    - Optimizador: `torch.optim.SGD`\n",
        "    - Tasa de aprendizaje: 0.01\n",
        "\n",
        "    3. Comparar los resultados obtenidos con los del entrenamiento anterior (implementaci√≥n manual). Algunas preguntas a responder:\n",
        "    - ¬øLa convergencia es m√°s r√°pida o m√°s lenta?\n",
        "    - ¬øC√≥mo se comporta la p√©rdida durante el entrenamiento?\n",
        "    - ¬øCu√°l implementaci√≥n fue m√°s f√°cil de modificar o extender?\n",
        "\n",
        "    4. Graficar la evoluci√≥n de la **p√©rdida** y el **accuracy** durante las √©pocas para ambas implementaciones (manual y PyTorch), idealmente en la misma figura para facilitar la comparaci√≥n. Pod√©s aplicar una media m√≥vil para suavizar la se√±al.\n",
        "\n",
        "    > üí° **Sugerencia pedag√≥gica**: antes de realizar este inciso, se recomienda repasar los notebooks `9a` y `9b`, donde se presentan una introducci√≥n a los tensores y al workflow de ML usando PyTorch.\n",
        "\n",
        "\n",
        "Cr√©dito: este ejercicio se base en la propuesta de Joe Sasson publicada en [Towards Data Science](https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKfhFNTxfEXm"
      },
      "source": [
        "### C√≥digo completo (Implementaci√≥n con Numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNxm6ujifEXm"
      },
      "outputs": [],
      "source": [
        "class Capa:\n",
        "    def __init__(self, neuronas):\n",
        "        self.neuronas = neuronas\n",
        "\n",
        "    def forward(self, inputs, weights, bias, activation):\n",
        "        \"\"\"\n",
        "        Forward Propagation de la capa\n",
        "        \"\"\"\n",
        "        Z_curr = np.dot(inputs, weights.T) + bias\n",
        "\n",
        "        if activation == 'relu':\n",
        "            A_curr = self.relu(inputs=Z_curr)\n",
        "        elif activation == 'softmax':\n",
        "            A_curr = self.softmax(inputs=Z_curr)\n",
        "\n",
        "        return A_curr, Z_curr\n",
        "\n",
        "    def relu(self, inputs):\n",
        "        \"\"\"\n",
        "        ReLU: funci√≥n de activaci√≥n\n",
        "        \"\"\"\n",
        "\n",
        "        return np.maximum(0, inputs)\n",
        "\n",
        "    def softmax(self, inputs):\n",
        "        \"\"\"\n",
        "        Softmax: funci√≥n de activaci√≥n\n",
        "        \"\"\"\n",
        "        exp_scores = np.exp(inputs)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
        "        \"\"\"\n",
        "        Backward Propagation de la capa\n",
        "        \"\"\"\n",
        "        if activation == 'softmax':\n",
        "            dW = np.dot(A_prev.T, dA_curr)\n",
        "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
        "            dA = np.dot(dA_curr, W_curr)\n",
        "        else:\n",
        "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
        "            dW = np.dot(A_prev.T, dZ)\n",
        "            db = np.sum(dZ, axis=0, keepdims=True)\n",
        "            dA = np.dot(dZ, W_curr)\n",
        "\n",
        "        return dA, dW, db\n",
        "\n",
        "    def relu_derivative(self, dA, Z):\n",
        "        \"\"\"\n",
        "        ReLU: gradiente de ReLU\n",
        "        \"\"\"\n",
        "        dZ = np.array(dA, copy = True)\n",
        "        dZ[Z <= 0] = 0\n",
        "        return dZ\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxBV0lcMfEXn"
      },
      "outputs": [],
      "source": [
        "class RedNeuronal:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.red = [] ## capas\n",
        "        self.arquitectura = [] ## mapeo de entradas -> salidas\n",
        "        self.pesos = [] ## W, b\n",
        "        self.memoria = [] ## Z, A\n",
        "        self.gradientes = [] ## dW, db\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def add(self, capa):\n",
        "        \"\"\"\n",
        "        Agregar capa a la red\n",
        "        \"\"\"\n",
        "        self.red.append(capa)\n",
        "\n",
        "    def _compile(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar la arquitectura\n",
        "        \"\"\"\n",
        "        for idx, _ in enumerate(self.red):\n",
        "            if idx == 0:\n",
        "                self.arquitectura.append({'input_dim': data.shape[1],\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'relu'})\n",
        "            elif idx > 0 and idx < len(self.red)-1:\n",
        "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas,\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'relu'})\n",
        "            else:\n",
        "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas,\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'softmax'})\n",
        "        return self\n",
        "\n",
        "    def _init_weights(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar arquitectura y los pesos\n",
        "        \"\"\"\n",
        "        self._compile(data)\n",
        "\n",
        "        np.random.seed(99)\n",
        "\n",
        "        for i in range(len(self.arquitectura)):\n",
        "            self.pesos.append({\n",
        "                'W':np.random.uniform(low=-1, high=1,\n",
        "                        size=(self.arquitectura[i]['input_dim'],\n",
        "                            self.arquitectura[i]['output_dim']\n",
        "                            )),\n",
        "                'b':np.zeros((1, self.arquitectura[i]['output_dim']))})\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _forwardprop(self, data):\n",
        "        \"\"\"\n",
        "        Pasada forward completa por la red\n",
        "        \"\"\"\n",
        "        A_curr = data\n",
        "\n",
        "        for i in range(len(self.pesos)):\n",
        "            A_prev = A_curr\n",
        "            A_curr, Z_curr = self.red[i].forward(inputs=A_prev,\n",
        "                                                    weights=self.pesos[i]['W'].T,\n",
        "                                                    bias=self.pesos[i]['b'],\n",
        "                                                    activation=self.arquitectura[i]['activation'])\n",
        "\n",
        "            self.memoria.append({'inputs':A_prev, 'Z':Z_curr})\n",
        "\n",
        "        return A_curr\n",
        "\n",
        "    def _backprop(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Pasada backward completa por la red\n",
        "        \"\"\"\n",
        "        num_samples = len(actual)\n",
        "\n",
        "        ## compute the gradient on predictions\n",
        "        dscores = predicted\n",
        "        dscores[range(num_samples),actual] -= 1\n",
        "        dscores /= num_samples\n",
        "\n",
        "        dA_prev = dscores\n",
        "\n",
        "        for idx, layer in reversed(list(enumerate(self.red))):\n",
        "            dA_curr = dA_prev\n",
        "\n",
        "            A_prev = self.memoria[idx]['inputs']\n",
        "            Z_curr = self.memoria[idx]['Z']\n",
        "            W_curr = self.pesos[idx]['W']\n",
        "\n",
        "            activation = self.arquitectura[idx]['activation']\n",
        "\n",
        "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr.T, Z_curr, A_prev, activation)\n",
        "\n",
        "            self.gradientes.append({'dW':dW_curr, 'db':db_curr})\n",
        "\n",
        "        self.gradientes = list(reversed(self.gradientes))  # Reverse the gradients list\n",
        "\n",
        "    def _update(self):\n",
        "        \"\"\"\n",
        "        Actualizar el modelo --> lr * gradiente\n",
        "        \"\"\"\n",
        "        lr = self.lr\n",
        "        for idx, layer in enumerate(self.red):\n",
        "            self.pesos[idx]['W'] -= lr * self.gradientes[idx]['dW']\n",
        "            self.pesos[idx]['b'] -= lr * self.gradientes[idx]['db']\n",
        "\n",
        "    def _get_accuracy(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calcular accuracy despu√©s de cada iteraci√≥n\n",
        "        \"\"\"\n",
        "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
        "\n",
        "    def _calculate_loss(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calculate cross-entropy loss after each iteration\n",
        "        \"\"\"\n",
        "        samples = len(actual)\n",
        "\n",
        "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
        "        data_loss = np.sum(correct_logprobs)/samples\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    def train(self, X_train, y_train, epochs):\n",
        "        \"\"\"\n",
        "        Entrenar el modelo Stochastic Gradient Descent\n",
        "        \"\"\"\n",
        "        self.loss = []\n",
        "        self.accuracy = []\n",
        "\n",
        "        self._init_weights(X_train)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            yhat = self._forwardprop(X_train)\n",
        "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
        "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
        "\n",
        "            self._backprop(predicted=yhat, actual=y_train)\n",
        "\n",
        "            self._update()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
        "                print(s)\n",
        "\n",
        "        return (self.accuracy, self.loss)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}